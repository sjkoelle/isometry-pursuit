
@ARTICLE{Kohli2021-lr,
  title    = "{LDLE}: Low Distortion Local Eigenmaps",
  author   = "Kohli, Dhruv and Cloninger, Alexander and Mishne, Gal",
  abstract = "We present Low Distortion Local Eigenmaps (LDLE), a manifold
              learning technique which constructs a set of low distortion local
              views of a data set in lower dimension and registers them to
              obtain a global embedding. The local views are constructed using
              the global eigenvectors of the graph Laplacian and are registered
              using Procrustes analysis. The choice of these eigenvectors may
              vary across the regions. In contrast to existing techniques, LDLE
              can embed closed and non-orientable manifolds into their
              intrinsic dimension by tearing them apart. It also provides
              gluing instruction on the boundary of the torn embedding to help
              identify the topology of the original manifold. Our experimental
              results will show that LDLE largely preserved distances up to a
              constant scale while other techniques produced higher distortion.
              We also demonstrate that LDLE produces high quality embeddings
              even when the data is noisy or sparse.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  22,
  year     =  2021,
  keywords = "closed manifold; graph laplacian; local parameterization;
              manifold learning; non-orientable manifold; procrustes analysis",
  language = "en"
}

@inproceedings{NEURIPS2019_6a10bbd4,
 author = {Chen, Yu-Chia and Meila, Marina},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Selecting the independent coordinates of manifolds with large aspect ratios},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Fazel2001ARM,
  title={A rank minimization heuristic with application to minimum order system approximation},
  author={Maryam Fazel and Haitham A. Hindi and Stephen P. Boyd},
  journal={Proceedings of the 2001 American Control Conference. (Cat. No.01CH37148)},
  year={2001},
  volume={6},
  pages={4734-4739 vol.6},
  url={https://api.semanticscholar.org/CorpusID:6000077}
}

@ARTICLE{Qin2013-tx,
  title     = "Efficient block-coordinate descent algorithms for the Group
               Lasso",
  author    = "Qin, Zhiwei and Scheinberg, Katya and Goldfarb, Donald",
  journal   = "Math. Program. Comput.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  5,
  number    =  2,
  pages     = "143--169",
  month     =  jun,
  year      =  2013,
  language  = "en"
}


@BOOK{Boyd2004-ql,
  title     = "Convex Optimization",
  author    = "Boyd, Stephen P and Vandenberghe, Lieven",
  abstract  = "Convex optimization problems arise frequently in many different
               fields. This book provides a comprehensive introduction to the
               subject, and shows in detail how such problems can be solved
               numerically with great efficiency. The book begins with the
               basic elements of convex sets and functions, and then describes
               various classes of convex optimization problems. Duality and
               approximation techniques are then covered, as are statistical
               estimation techniques. Various geometrical problems are then
               presented, and there is detailed discussion of unconstrained and
               constrained minimization problems, and interior-point methods.
               The focus of the book is on recognizing convex optimization
               problems and then finding the most appropriate technique for
               solving them. It contains many worked examples and homework
               exercises and will appeal to students, researchers and
               practitioners in fields such as engineering, computer science,
               mathematics, statistics, finance and economics.",
  publisher = "Cambridge University Press",
  month     =  mar,
  year      =  2004,
  language  = "en"
}



@ARTICLE{Zhao2023-xn,
  title         = "A Survey of Numerical Algorithms that can Solve the Lasso
                   Problems",
  author        = "Zhao, Yujie and Huo, Xiaoming",
  abstract      = "In statistics, the least absolute shrinkage and selection
                   operator (Lasso) is a regression method that performs both
                   variable selection and regularization. There is a lot of
                   literature available, discussing the statistical properties
                   of the regression coefficients estimated by the Lasso
                   method. However, there lacks a comprehensive review
                   discussing the algorithms to solve the optimization problem
                   in Lasso. In this review, we summarize five representative
                   algorithms to optimize the objective function in Lasso,
                   including the iterative shrinkage threshold algorithm
                   (ISTA), fast iterative shrinkage-thresholding algorithms
                   (FISTA), coordinate gradient descent algorithm (CGDA),
                   smooth L1 algorithm (SLA), and path following algorithm
                   (PFA). Additionally, we also compare their convergence rate,
                   as well as their potential strengths and weakness.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "2303.03576"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Meier2008-ts,
  title    = "The group lasso for logistic regression",
  author   = "Meier, L and van de Geer, Sara and B{\"u}hlmann, P",
  abstract = "Summary. The group lasso is an extension of the lasso to do
              variable selection on (predefined) groups of variables in linear
              regression models. The estimates have the attractive property of
              being invariant under groupwise orthogonal reparameterizations.
              We extend the group lasso to logistic regression models and
              present an efficient algorithm, that is especially suitable for
              high dimensional problems, which can also be applied to
              generalized linear models to solve the corresponding convex
              optimization problem. The group lasso estimator for logistic
              regression is shown to be statistically consistent even if the
              number of predictors is much larger than sample size but with
              sparse true underlying structure. We further use a two‐stage
              procedure which aims for sparser models than the group lasso,
              leading to improved prediction performance for some cases.
              Moreover, owing to the two‐stage nature, the estimates can be
              constructed to be hierarchical. The methods are used on simulated
              and real data sets about splice site detection in DNA sequences.",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume   =  70,
  month    =  feb,
  year     =  2008
}


@ARTICLE{Bertsimas2022-qo,
  title    = "Sparse {PCA}: A geometric approach",
  author   = "Bertsimas, D and Kitane, Driss Lahlou",
  abstract = "We consider the problem of maximizing the variance explained from
              a data matrix using orthogonal sparse principal components that
              have a support of fixed cardinality. While most existing methods
              focus on building principal components (PCs) iteratively through
              deflation, we propose GeoSPCA, a novel algorithm to build all PCs
              at once while satisfying the orthogonality constraints which
              brings substantial benefits over deflation. This novel approach
              is based on the left eigenvalues of the covariance matrix which
              helps circumvent the non-convexity of the problem by
              approximating the optimal solution using a binary linear
              optimization problem that can find the optimal solution. The
              resulting approximation can be used to tackle different versions
              of the sparse PCA problem including the case in which the
              principal components share the same support or have disjoint
              supports and the Structured Sparse PCA problem. We also propose
              optimality bounds and illustrate the benefits of GeoSPCA in
              selected real world problems both in terms of explained variance,
              sparsity and tractability. Improvements vs. the greedy algorithm,
              which is often at par with state-of-the-art techniques, reaches
              up to 24\% in terms of variance while solving real world problems
              with 10,000s of variables and support cardinality of 100s in
              minutes. We also apply GeoSPCA in a face recognition problem
              yielding more than 10\% improvement vs. other PCA based technique
              such as structured sparse PCA.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  24,
  pages    = "32:1--32:33",
  month    =  oct,
  year     =  2022
}

@ARTICLE{Hastie2015-qa,
  title    = "Statistical Learning with sparsity: The lasso and Generalizations",
  author   = "Hastie, T and Tibshirani, R and Wainwright, M",
  abstract = "Discover New Methods for Dealing with High-Dimensional Data A
              sparse statistical model has only a small number of nonzero
              parameters or weights; therefore, it is much easier to estimate
              and interpret than a dense model. Statistical Learning with
              Sparsity: The Lasso and Generalizations presents methods that
              exploit sparsity to help recover the underlying signal in a set
              of data. Top experts in this rapidly evolving field, the authors
              describe the lasso for linear regression and a simple coordinate
              descent algorithm for its computation. They discuss the
              application of 1 penalties to generalized linear models and
              support vector machines, cover generalized penalties such as the
              elastic net and group lasso, and review numerical methods for
              optimization. They also present statistical inference methods for
              fitted (lasso) models, including the bootstrap, Bayesian methods,
              and recently developed approaches. In addition, the book examines
              matrix decomposition, sparse multivariate analysis, graphical
              models, and compressed sensing. It concludes with a survey of
              theoretical results for the lasso. In this age of big data, the
              number of features measured on a person or object can be large
              and might be larger than the number of observations. This book
              shows how the sparsity assumption allows us to tackle these
              problems and extract useful and reproducible patterns from big
              datasets. Data analysts, computer scientists, and theorists will
              appreciate this thorough and up-to-date treatment of sparse
              statistical modeling.",
  month    =  may,
  year     =  2015
}


@ARTICLE{Chen2001-hh,
  title    = "Atomic Decomposition by Basis Pursuit",
  author   = "{Scott Shaobing Chen and David L. Donoho and Michael A. Saunders}",
  abstract = "The time-frequency and time-scale communities have recently
              developed a large number of overcomplete waveform
              dictionaries---stationary wavelets, wavelet packets, cosine
              packets, chirplets, and warplets, to name a few. Decomposition
              into overcomplete systems is not unique, and several methods for
              decomposition have been proposed, including the method of frames
              (MOF), matching pursuit (MP), and, for special dictionaries, the
              best orthogonal basis (BOB). Basis pursuit (BP) is a principle
              for decomposing a signal into an ``optimal'' superposition of
              dictionary elements, where optimal means having the smallest l 1
              norm of coefficients among all such decompositions. We give
              examples exhibiting several advantages over MOF, MP, and BOB,
              including better sparsity and superresolution. BP has interesting
              relations to ideas in areas as diverse as ill-posed problems,
              abstract harmonic analysis, total variation denoising, and
              multiscale edge denoising. BP in highly overcomplete dictionaries
              leads to large-scale optimization problems. With signals of
              length 8192 and a wavelet packet dictionary, one gets an
              equivalent linear program of size 8192 by 212,992. Such problems
              can be attacked successfully only because of recent advances in
              linear and quadratic programming by interior-point methods. We
              obtain reasonable success with a primal-dual logarithmic barrier
              method and conjugategradient solver.",
  journal  = "SIAM REVIEW",
  volume   =  43,
  number   =  1,
  pages    = "129",
  month    =  feb,
  year     =  2001
}


@ARTICLE{Sun2012-vp,
  title         = "Sparse Matrix Inversion with Scaled Lasso",
  author        = "Sun, Tingni and Zhang, Cun-Hui",
  abstract      = "We propose a new method of learning a sparse
                   nonnegative-definite target matrix. Our primary example of
                   the target matrix is the inverse of a population covariance
                   or correlation matrix. The algorithm first estimates each
                   column of the target matrix by the scaled Lasso and then
                   adjusts the matrix estimator to be symmetric. The penalty
                   level of the scaled Lasso for each column is completely
                   determined by data via convex minimization, without using
                   cross-validation. We prove that this scaled Lasso method
                   guarantees the fastest proven rate of convergence in the
                   spectrum norm under conditions of weaker form than those in
                   the existing analyses of other $\ell_1$ regularized
                   algorithms, and has faster guaranteed rate of convergence
                   when the ratio of the $\ell_1$ and spectrum norms of the
                   target inverse matrix diverges to infinity. A simulation
                   study demonstrates the computational feasibility and superb
                   performance of the proposed method. Our analysis also
                   provides new performance bounds for the Lasso and scaled
                   Lasso to guarantee higher concentration of the error at a
                   smaller threshold level than previous analyses, and to allow
                   the use of the union bound in column-by-column applications
                   of the scaled Lasso without an adjustment of the penalty
                   level. In addition, the least squares estimation after the
                   scaled Lasso selection is considered and proven to guarantee
                   performance bounds similar to that of the scaled Lasso.",
  month         =  feb,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1202.2723"
}


@ARTICLE{Jones2007-uc,
  title         = "Universal local parametrizations via heat kernels and
                   eigenfunctions of the Laplacian",
  author        = "Jones, Peter W and Maggioni, Mauro and Schul, Raanan",
  abstract      = "We use heat kernels or eigenfunctions of the Laplacian to
                   construct local coordinates on large classes of Euclidean
                   domains and Riemannian manifolds (not necessarily smooth,
                   e.g. with $\mathcal\{C\}^\alpha$ metric). These coordinates
                   are bi-Lipschitz on embedded balls of the domain or
                   manifold, with distortion constants that depend only on
                   natural geometric properties of the domain or manifold. The
                   proof of these results relies on estimates, from above and
                   below, for the heat kernel and its gradient, as well as for
                   the eigenfunctions of the Laplacian and their gradient.
                   These estimates hold in the non-smooth category, and are
                   stable with respect to perturbations within this category.
                   Finally, these coordinate systems are intrinsic and
                   efficiently computable, and are of value in applications.",
  month         =  sep,
  year          =  2007,
  archivePrefix = "arXiv",
  primaryClass  = "math.AP",
  eprint        = "0709.1975"
}


@ARTICLE{Chen2019-km,
  title    = "Selecting the independent coordinates of manifolds with large
              aspect ratios",
  author   = "Chen, Yu-Chia and Meil{\u a}, M",
  abstract = "Many manifold embedding algorithms fail apparently when the data
              manifold has a large aspect ratio (such as a long, thin strip).
              Here, we formulate success and failure in terms of finding a
              smooth embedding, showing also that the problem is pervasive and
              more complex than previously recognized. Mathematically, success
              is possible under very broad conditions, provided that embedding
              is done by carefully selected eigenfunctions of the
              Laplace-Beltrami operator $\Delta$. Hence, we propose a
              bicriterial Independent Eigencoordinate Selection (IES) algorithm
              that selects smooth embeddings with few eigenvectors. The
              algorithm is grounded in theory, has low computational overhead,
              and is successful on synthetic and large real data.",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   = "abs/1907.01651",
  month    =  jul,
  year     =  2019
}

@inproceedings{He2023-ch,
  title     = "Product Manifold Learning with Independent Coordinate Selection",
  author    = "He, Jesse and Brug{\`e}re, Tristan and Mishne, Gal",
  booktitle = "Proceedings of the 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at ICML",
  abstract  = "In many dimensionality reduction tasks, we wish to identify the
               constituent components that explain our observations. For
               manifold learning, this can be formalized as factoring a
               Riemannian product manifold. Recovering this factorization,
               however, may suffer from certain difficulties in practice,
               especially when data is sparse or noisy, or when one factor is
               distorted by the other. To address these limitations, we propose
               identifying non-redundant coordinates on the product manifold
               before applying product manifold learning to identify which
               coordinates correspond to different factor manifolds. We
               demonstrate our approach on both synthetic and real-world data.",
  month     = jun,
  year      = 2023
}


@INPROCEEDINGS{Koelle2024-no,
  title     = "Consistency of dictionary-based manifold learning",
  booktitle = "Proceedings of The 27th International Conference on Artificial
               Intelligence and Statistics",
  author    = "Koelle, Samson J and Zhang, Hanyu and Murad, Octavian-Vlad and
               Meila, Marina",
  editor    = "Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen",
  abstract  = "We analyze a paradigm for interpretable Manifold Learning for
               scientific data analysis, whereby one parametrizes a manifold
               with d smooth functions from a scientist-provided dictionary of
               meaningful, domain-related functions. When such a
               parametrization exists, we provide an algorithm for finding it
               based on sparse regression in the manifold tangent bundle,
               bypassing more standard, agnostic manifold learning algorithms.
               We prove conditions for the existence of such parameterizations
               in function space and the first end to end recovery results from
               finite samples. The method is demonstrated on both synthetic
               problems and with data from a real scientific domain.",
  publisher = "PMLR",
  volume    =  238,
  pages     = "4348--4356",
  series    = "Proceedings of Machine Learning Research",
  year      =  2024
}



@ARTICLE{Koelle2022-ju,
  title    = "Manifold Coordinates with Physical Meaning",
  author   = "Koelle, Samson J and Zhang, Hanyu and Meila, Marina and Chen,
              Yu-Chia",
  journal  = "J. Mach. Learn. Res.",
  volume   =  23,
  number   =  133,
  pages    = "1--57",
  year     =  2022
}
