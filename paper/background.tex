\section{Background}

\subsection{Problem}

Our goal is, given a matrix $\mathcal X \in \mathbb R^{D \times P}$, select a subset $\mathcal S \subset [P]$ with $|\mathcal S| = D$ such that $X_{. \mathcal S}$ is as unitary as possible in a computationally efficient way.
To that end, define a ground truth loss function that measures unitariness, and then introduce a surrogate loss function that convexifies the problem so that it may be efficiently solved.

\subsection{Techniques}

\subsubsection{Unitary matrices}

\begin{definition}[Unitary]
A rank $D$ matrix $U \in \mathbb R^{D_\alpha \times D_\beta}$ is said to be \textbf{unitary} if $U^TU = I_D$.
\end{definition}

While this definition implies $D_\alpha, D_\beta \geq D$, this paper typically works with equality.
Even so, this paper uses the term singular values rather than eigenvalues, and Section \ref{sec:discussion} contains speculation that a similar set of techniques hold for rank-d unitary rectangular matrices.

The property of unitary matrices that motivates our ground truth loss function comes from spectral analysis.
\begin{proposition}[Singular values of a unitary matrix]
\label{prop:unitary_spectrum}
The singular values $\sigma_1 \dots \sigma_D$ are equal to $1$ if and only if $U \in \mathbb{R}^{D_\alpha \times D_\beta}$ is unitary.
\end{proposition}

As a ground truth loss function, we'd like the loss to be minimized uniquely by unitary matrices, invariant under rotation, and depend on all changes in the matrix.
The first desired property precludes, for example, the log determinant, while the last precludes the operator norm, also known as the deformation. %check
Finally, to simplify our overall exposition and exemplify the key features of the method, we'd like this ground truth loss to be as similar as possible to the convex loss we introduce.
In fact, these losses will behave equivalently over diagonalizable matrices.
That is, in a sense, the ground truth loss will be as convex as possible.
%Thus, in Section \ref{sec:ground_truth}, we introduce a new loss based on the LogSumExp function.

The equivalent property of unitary matrices that we use to define our alternative convex loss is slightly different from Proposition \ref{prop:unitary_spectrum}.
%cite 
\begin{proposition}[Basis vectors of a unitary matrix]
\label{prop:unitary_basis}
The component vectors $u^1 \dots u^D \in \mathbb R^B$ form a unitary matrix if and only if, for all $d_1, d_2 \in [D], u_{d_1} u^{d_2} = \begin{cases}
1 \; d_1 = d_2\\ 
0 \; d_1 \neq d_2 
\end{cases}$.
\end{proposition}

We will show that the conditions of the antecedent of Proposition \ref{prop:unitary_basis} are satisfied by the solutions to the multitask basis pursuit problem applied to matrices consisting of suitably normalized vectors.
The vectors are in particular length normalized so that the post-normalization vectors with the longest length have length $1$.
As we show in Proposition , our main theoretical result,  the tendency of the multitask basis pursuit problem to select orthogonal features then ensures that a unitary matrix is deterministically selected, given that one is present.
%explain that multitask and group are almost equivalent
% if and only if is an equality in the logical statement (assertion, theorem) monoid.

%While other properties like the operator norm
%The nuclear norm is not 

\subsubsection{Basis pursuit and lasso}

% NOTE (Sam): find a citation for multitask basis pursuit.  Surely, this has been done before.

We use "multitask" instead of "group" to refer to this algorithm, as grouping is entirely determined by the dependent variable.

Basis pursuit programs are of the form 
\begin{align}
\min c(\beta) : d(y, x\beta) \leq o
\end{align}

Basis pursuit programs are convex, and solvable via standard interior point methods.

On the other hand, the basis pursuit program admits 

\subsubsection{Isometries}

Another motivating example of this paper is the embedding of non-linear data manifolds by selecting functions from within a dictionary that preserve distances and angles.
These sets of functions are called isometries if they do so globally and local isometries if they do so almost everywhere.
If more pointwise specificity is required, they are known at isometric at a point.
To avoid abstractness, we give the following definition in coordinates.
\begin{definition}{Isometric at a point}
\label{def:isometric_at_a_point}
A map $\phi$ between $D$ dimensional submanifolds with inherited Euclidean metric $\mathcal M \subseteq R^{B_\alpha}$ and $\mathcal N  \subseteq R^{B_\beta}$ is an \textbf{isometry at a point} $p \in \mathcal M$ if
\begin{align}
\| v^T D \phi (p) \| = \|v\| \forall v \in \mathbb R^D
\end{align}
$D \phi$ is the differential implicitly given by $D_{F}^E \phi = U D \phi V^T$ where $U$ and $V$ are bases for $(T_p \mathcal M)$ and $T_{\phi(p)} \mathcal N$, the tangent spaces of $\mathbb R$ and $\mathcal N$ at $p$ and $\phi(p)$, respectively.
% .  A Grassmannian.  (what is a Stiefel manifold?).
\end{definition}

The reasons that pointwise isometry is interesting are themselves manifold.
\cite{Kohli2021-lr} showed how pointwise isometries selected from a dictionary may be stitched together to form global embeddings.
Another line of work \cite{Koelle2022-ju, Koelle2024-no} has shown how independent functions may be selected globally using group lasso, and that this selection process has implications for the metric properties of the selected embedding \cite{Koelle2022-ju}.
Other approaches like Local Tangent Space Alignment seek to identify isometric coordinates non-parametrically through tangent space estimation prior to stitching, while multidimensional scaling and isomap do so directly through preservation of distances.
A classic result in differential geometry equilibrates these two approaches.
% NOTE (Sam): which came first?

Given the metric characterization of isometry in Definition \ref{def:isometric_at_a_point}, is it appropriate to recall another equivalent characterization of unitary matrices.
\begin{proposition}[Metric properties of a unitary matrix]
\label{prop:unitary_spectrum}
$U \in \mathbb{R}^{D_\alpha \times D_\beta}$ is unitary if and only if $\| v^T  U$ for all $v \in \mathbb R^{D_\beta}$.
\end{proposition}
Thus, in coordinates, $D \phi (p)$ is unitary if $\phi$ is isometric at a point $p$.

%Note that in the case of tabular data, where $\mathcal M = \mathbb R^P$, and $\mathcal N = \mathbb R^D$, and $\phi \in \mathbb R^{D \times P}$ is a linear map, this is equivalent to saying that $\phi$ is unitary.
$X$ could be, for example, the Jacobian matrix $d g$ of a set of candidate coordinate functions $g = [g^1, \dotsc g^P]$.

% We assume that we are given a target dimension $T \leq D$ - do we?
% BIG QUESTION: RUN IN D = B?  What is rank of $X$?    It's D if not running D = B



\subsection{Applications}

\subsubsection{Diversification}

Say you are hosting an elegant dinner party.

\subsubsection{Embedding}

\subsubsection{Interpretability}





