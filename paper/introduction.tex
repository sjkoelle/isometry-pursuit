\section{Introduction}
\label{sec:introduction}

Many real-world problems may be abstracted as selecting a subset of the columns of a matrix representing stochastic observations or analytically exact data.
This paper focuses on a simple such problem that appears in unsupervised learning.
Given a rank $D$ matrix $\mathcal X \in \mathbb R^{D \times P}$ with $P > D$, select a square submatrix $\mathcal X_{.\mathcal S}$ where subset $\mathcal S \subset P$ satisfies $|\mathcal S| = D$ that is as unitary as possible.

This problem is motivated by applications in diversification and non-linear dimension reduction.
In particular, the name of the method comes from the fact that isometric embeddings have unitary differentials.
While variable selection in unsupervised learning is comparably less studied than in supervised learning, substantial literature exists.
One method that exemplifies this area is Sparse PCA \cite{Dey2017-mx}, in which a subset of variables are used to generate low-dimensional projections.
% compared with sparse pca loadings are constrained to be 1.  is it convex?
Within non-linear dimension reduction dictionaries can be either given \cite{Koelle2022-ju, Koelle2024-no} or learned \cite{Kohli2021-lr}. 
In order of specificity, these methods may seek to optimize independent coordinates \cite{Chen2019-km, He2023-ch}, low distortion embeddings, or isometric embeddings.
Optimization can be global or local.
These coordinate selection algorithms can be greedy \cite{NEURIPS2019_6a10bbd4, Kohli2021-lr, Jones2007-uc} or convex \cite{Koelle2022-ju, Koelle2024-no}.
%% cite sparse 
%Many of these operate locally.

%(RAG, recommendations, training neural networks (infrequent meta), wind).

The insight that leads to isometry pursuit is that $D$ function solutions multitask basis pursuit applied to an appropriately normalized $\mathcal X$ selects unitary submatrices.
%This approach relies on a to-our-knowledge novel matrix inversion algorithm that is sparse in the column space of the matrix.
This normalization is log-symmetric length in the column-space of $\mathcal X$ and favors vectors closer to unit length.
This property is the focus of Section \ref{sec:theory}

The basis pursuit formulation is desirable for several reasons.
First, it is convex and therefore computationally expedient.
Second, while not $D$-sparse, it is relatively sparse, and so can be used for pruning. % we need more theory on the D subset within the retained subset, need to compare brute force and trimmed solutions.
Third, it admits a lasso dual problem which is particularly useful in high dimensions.

%These problems are solvable with off-the-shelf multitask lasso and interior point solvers, respectively.
% Square selection could be relaxed somewhat.

%Unsupervised learning methods like PCA, UMAP, and autoencoders are often concerned with minimizing reconstruction error without regard for the sparsity of the learned respresentation, and among those that sparsify, variable selection methods contrast with sparsification with respect to reconstruction from a learned latent space.

% Multitask learning and lasso \cite{Hastie2015-qa}

%Within the context of non-linear dimension reduction, selection of coordinate functions of an embedding space from within a dictionary is a core problem in geometric data analysis.  % more on interpretability

%In this paper we show that an adapted version of the group lasso type algorithm in \cite{Koelle2024-no} leads to a convex procedure competitive with previous greedy approaches with respect to isometry.

% cite some classic manifold learning papers

%The comparison of greedy (e.g. Orthogonal Matching Pursuit) \cite{Mallat93-wi, Tropp05-ml} and convex \cite{Tropp06-sg} basis pursuit formulations.

%Basis pursuit \cite{Chen2001-hh} 
% Recommendation, RAG

% Cite dominique and marina.