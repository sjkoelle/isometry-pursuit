\section{Supplement}
% NOTE (Sam): make sure to add basis pursuit equivalence
We give proofs in support of the propositions in the main text and supplemental experimental information to better contextualize the results. 

\subsection{Proofs}
\label{sec:proofs}

\subsubsection{Proof of Proposition \ref{prop:basis_pursuit_selection_invariance}}
\label{proof:basis_pursuit_program_invariance}

In this proof we first show that the penalty $\|\beta\|_{1,2}$ is unchanged by unitary transformation of $\beta$.

 \begin{proposition}{Loss equivalence}
 \label{prop:basis_pursuit_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\|\beta\|_{1,2} = \|\beta U \|$.
\end{proposition}

\begin{proof}
\begin{align}
\|\beta U \|_{1,2} &= \sum_{p = 1}^P \| \beta_{p.} U \| \\
&= \sum_{p = 1}^P \| \beta_{p.} \| \\
&= \|\beta \|_{1,2}
\end{align}
\end{proof}

We then show that this implies that the resultant loss is unchanged by unitary transformation of $\mathcal X$.

\begin{proposition}
 \label{prop:basis_pursuit_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\widehat \beta  (U \mathcal X) = \widehat \beta  ( \mathcal X) U$.
\end{proposition}

\begin{proof}
\begin{align}
\widehat \beta  (U \mathcal X)  &= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \; I_{D} = U X \beta \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \; U^{-1} U = U^{-1} U X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \;  I_D = X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta U \|_{1,2}  \; : \;  I_D = X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta \|_{1,2}  \; : \;  I_D = X \beta.
\end{align}
\end{proof}

\subsubsection{Proof of Proposition \ref{prop:unitary_selection}}
\label{sec:local_isometry_proof}

 \begin{proposition}[Unitary selection]
\label{prop:generalized_unitary_selection}
Let $w_c$ be a normalization satisfying the conditions in Definition \ref{eq:symmetric_normalization}.  Then $\arg \min_{X_{.S} \in \mathbb R^{D \times D}} \widehat \beta^{D}_c (X_{.S]) $ is orthonormal.  Moreover when $X$ is orthonormal, $\min_{\beta \in \mathbb R^{P \times D}} \| \beta \|_{1,2} \; : \; I_D = w ({ \mathcal X}, c) \beta = D$.
 \end{proposition}
 
 \begin{proof}

The value of $D$ is clearly obtained by $\beta$ orthonormal, since by Proposition \ref{basis_pursuit_selection_invariance}, for $X$ orthogonal, without loss of generality 
\begin{align}
\beta_{jk} = \begin{cases} 1 & j = k \in \{ 1 \dots d\}  \\
0 & \text{otherwise}
\end{cases}.
\end{eqnarray}
Thus, we need to show that this is a lower bound on the obtained loss and is only obtained by unitary matrices.

From the conditions in Definition \ref{eq:symmetric_normalization}, normalized matrices will consist of vectors of maximum length (i.e. $1$) if and only if the original matrix consists of vectors of length.  These vectors will clearly result in lower basis pursuit loss.

Therefore, it remains to show that orthogonal vectors of length $1$ reduce loss over non-orthogonal vectors.
Invertible matrices $X_{.S}$ admit QR decompositions $\tilde X_{.S} = QR$ where $Q$ and $R$ are square unitary and upper-triangular matrices, respectively \citep{Anderson1992-fb}.
Since $l^*$ is invariant to unitary transformations, we can without loss of generality, consider $Q= I_d$.
Denoting $I_d$ to be composed of basis vectors $[e^1 \dots e^d]$, the matrix $R$ has form
\begin{align}
R = \begin{bmatrix}
\langle e^1, \tilde X_{i.{S_1}} \rangle & \langle e^1, \tilde X_{i.{S_2}} \rangle  &\dots &  \langle e^1, \tilde X_{i.{S_d}} \rangle \\
0 & \langle e^2, \tilde X_{i.{S_2}} \rangle & \dots  &  \langle e^2, \tilde X_{i.{S_d}} \rangle\\
0 & 0 & \dots & \dots  \\
0 & 0 & 0& \langle e^d, \tilde X_{i.{S_d}} \rangle 
\end{bmatrix}.
\end{align}
The diagonal entries $R_{jj} = \langle q^j, \tilde X_{i.{S_j}} \rangle$ of this matrix have form $\| \tilde X_{i.{S_j}} -  \sum_{j' \in \{1 \dots j-1\}} \langle \tilde X_{i.{S_{j}}}, e^{j'} \rangle e^{j'} \|$.
Thus, $R_{jj} \in (0, \| \tilde X_{i.{S_j}}\|]$.
On the other hand $\beta_{iS.} =R^{-1}$, which has diagonal elements $\beta_{jj} = R_{jj}^{-1}$, since $R$ is upper triangular.
Thus, $\beta_{ijj} \geq \| \tilde X_{i.{S_j}}\|^{-1}$, and therefore $\|\beta_{iS_j.}\| \geq \|\beta_{iS_j.}'\|.$
Since $\|\beta_{iS_j.}\| \geq \|\beta_{iS_j.}'\|$ for all $i$, then $\|\beta_{.S_j.}\| \geq \|\beta_{.S_j.}' \|$.
%Finally, that this is true for all $j$.
%That is, if we 
\end{proof}



\end{proof}