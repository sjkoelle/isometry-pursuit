\section{Supplement}
% NOTE (Sam): make sure to add basis pursuit equivalence
We give proofs in support of the propositions in the main text and supplemental experimental information to better contextualize the results. 

\subsection{Proofs}
\label{sec:proofs}

\subsubsection{Proof of Proposition \ref{prop:basis_pursuit_selection_invariance}}
\label{proof:basis_pursuit_program_invariance}

In this proof we first show that the penalty $\|\beta\|_{1,2}$ is unchanged by unitary transformation of $\beta$.

 \begin{proposition}{Loss equivalence}
 \label{prop:basis_pursuit_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\|\beta\|_{1,2} = \|\beta U \|$.
\end{proposition}

\begin{proof}
\begin{align}
\|\beta U \|_{1,2} &= \sum_{p = 1}^P \| \beta_{p.} U \| \\
&= \sum_{p = 1}^P \| \beta_{p.} \| \\
&= \|\beta \|_{1,2}
\end{align}
\end{proof}

We then show that this implies that the resultant loss is unchanged by unitary transformation of $\mathcal X$.

\begin{proposition}
 \label{prop:basis_pursuit_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\widehat \beta  (U \mathcal X) = \widehat \beta  ( \mathcal X) U$.
\end{proposition}

\begin{proof}
\begin{align}
\widehat \beta  (U \mathcal X)  &= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \; I_{D} = U X \beta \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \; U^{-1} U = U^{-1} U X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \;  I_D = X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta U \|_{1,2}  \; : \;  I_D = X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta \|_{1,2}  \; : \;  I_D = X \beta.
\end{align}
\end{proof}

\subsubsection{Proof of Proposition \ref{prop:unitary_selection}}
\label{sec:local_isometry_proof}

In support of the proposition in the main text, we give a more general theorem which illustrates the key features of the method.
From there, it is a straightforward step to specify to the chosen normalization.

 \begin{proposition}[Generalized unitary selection]
\label{prop:generalized_unitary_selection}
Given a matrix $\mathcal X \in \mathbb R^{D \times P}$ with a rank $D$ submatrix $\mathcal X_{.\mathcal S} \in \mathbb R^{D \times D}$ that is the unique $D \times D$ rank $D$-unitary submatrix, and let $n_q$ be a normalization composed of normalizing functions $q$ that satisfy the conditions of Definition \ref{def:symmetric_normalization}, then  $\mathcal S = S(\widehat{\beta}_{q}^P (\mathcal X)))$.
 \end{proposition}
 
 \begin{proof}
 
%The first component of this proof deals with $D$-sparse solutions only.  
%In this section, there are two main claims: that, given that a matrix consists of column-vectors of a certain set of lengths, loss will be minimized if these vectors are orthogonal, and that, given that a matrix consists of orthogonal columns, loss will be minimized if these vectors are of length one.
%These propositions suffice to show that, of all $D$-sparse solutions, the minimizer will be the unique unitary submatrix.
%The second step of the proof consists of the claim that $D$-sparse solutions are preferable to those consisting of more components.
%We proceed similarly, showing that loss is reduced by making any $D$-column submatrix of the putative solution orthonormal, and that loss is minimized further by removal of the non orthonormal components.

This proof consists of showing that replacing an arbitrary $D$ subset $\mathcal S$ with a unitary $D \times D$ matrix leads to existence of a solution which reduces loss, and that if such a subset already exists, loss may not be reduced any further.

\begin{proposition}[Unitary Preference]
Given a rank $D$ matrix $X \in \mathbb{R}^{D \times P}$, let $\beta$ be the solution to Program \ref{prog:isometry_pursuit}.
Select an arbitrary subset $\mathcal{S} \subseteq [P]$ with $|\mathcal{S}| = D$.
Define a new matrix $X' \in \mathbb{R}^{D \times P}$ as follows:
\[
X_{.p}' = \begin{cases} 
X_{.p} & \text{if } p \notin \mathcal{S}, \\
U_{.p} & \text{if } p \in \mathcal{S},
\end{cases}
\]
where $U \in \mathbb{R}^{D \times D}$ is any unitary matrix.
\end{proposition}

\begin{proof}
By the conditions in Definition  \ref{def:symmetric_normalization}, the maximum length for vectors $\mathcal X^q_{.p}$ is attained when $\|\mathcal X_{.p}\|_2 = 1$.
By Proposition \ref{prop:unitary_basis}, this is the case for unitary matrices, but it is also true for matrices for which $\mathcal X^q_{.p}$ are not orthogonal.
%To handle such matrices, we recall the following proposition.
%\begin{proposition}[Non-orthogonality]
%Given a rank $D$ unitary matrix $U \in \mathbb R^{D\times D}$ and a vector $v \in \mathbb R^D$, let $\gamma \in \mathbb R^D$ such that $v = U \gamma$.
%Then $\|\gamma\|_2 = \|v\|_2$.
% also need a prop about invariance of \gamma to choice of basis?
%\end{proposition}

\begin{proposition}[Support orthogonalization]
\label{prop:support_orthogonalization}
Let $\beta_S'$ be the solution to
\begin{align}
X_S \beta_S = X_S' \beta_S'.
\end{align}
Note that this is guaranteed to exist since $X_S'$ is full rank.  
Then $\|\beta_S\|_{1,2} \geq \|\beta_S'\|_{1,2}$
\end{proposition}
\begin{proof}
Since $X_S'$ is full rank we can define $\gamma_{SS} \in \mathbb R^{D \times D}$ such that 
\begin{align}
\label{eq:gamma_ss}
X_S  = X_S' \gamma_{SS}.
\end{align}
Then 
\begin{align}
X_S \beta_S &= X_S' \gamma_{SS} \beta_S
\end{align}
and so
\begin{align}
\gamma_{SS} \beta_S = \beta_S'.
\end{align}
Thus, we want to show that 
\begin{align}
\| \gamma_{SS} \beta_S  \|_{1,2} \leq \|\beta_S\|_{1,2}.
\end{align}

Since $\|X_S'\|$ is unitary, $\|X_S' \gamma_{Sp}\|_2 = \| \gamma_{Sp}\|_2$.
Therefore, Equation \ref{eq:gamma_ss} and $\|X_{.p}\|_2 \leq 1$ imply that $\|\gamma_{Sp}\|_2 \leq 1 \; \forall  p \in S$.
Thus, showing the proposition amounts to showing that the $\|\|_{1,2}$ norm is submultiplicative.

In lieu of showing directly, we show that $\|\|_{1,2}$ is subadditive and therefore, since it is unitary invariant by Proposition \ref{prop:basis_pursuit_loss_equivalence} and $\|\text{diag}(1, 0, \dotsc, 0)\|_{1,2} =1$ satisfies the conditions in \cite{HIAI2004155}, and thus is submultiplicative.

\begin{proposition}[Subadditivity of group lasso norm]
\label{prop:subadditivity}
Let $A, B \in \mathbb R^{D \times D}$.  Then $\|A + B \|_{1,2} \leq \|A  \|_{1,2}  +  \|B \|_{1,2} $
\end{proposition}
\begin{proof}
\begin{align}
\|A + B \|_{1,2} &= \sum_{d = 1}^D \|A_{d.} + B_{d.}\|_2 \\
&\leq \sum_{d = 1}^D \|A_{d.}\|_2 + \|B_{d.}\|_2 \\
&= \|A  \|_{1,2}  +  \|B \|_{1,2} 
\end{align}
\end{proof}

Thus, $\|\beta_S\|_{1,2} \geq \|\beta_S'\|_{1,2}$.

\end{proof}

The next component of the argument deals with elements of $[P]$ which are not contained within $\mathcal S$.
We show that zeroing components of $\beta_{p.}'$ where $p \not \in S$ and compensating for the removal by altering the loadings within $\mathcal S$ results in lower loss.

\begin{proposition}[Support replacement]
Let $\beta_S"$ be the solution to
\begin{align}
X_S' \beta_S ' + X_{S^C} \beta_{S^C} = X_S' \beta_S". % NOTE (Sam): need to clarify that \beta_{c.} is a vector
\end{align}
Note that this is guaranteed to exist since $X_S'$ is full rank.  
Then $\|\beta_{S}'\|_{1,2} + \|\beta_{S^C}'\|_2 \geq \|\beta_S"\|_{1,2}$
% NOTE (Sam): beta_c is not ' since its only operated on by the change from the earlier theorem.  Refitting would lead to a changed value.
\end{proposition}

\begin{proof}
Since $X_S'$ is full rank we can define $\gamma_{SS^C} \in \mathbb R^{D \times (P-D)}$ such that $X_{S^C} = X_S' \gamma_{SS^C}$.
Then 
\begin{align}
X_S' \beta_S" &= X_S' \beta_S ' + X_{S^C} \beta_{S^C} \\
&= X_S' \beta_S ' + X_S' \gamma_{SS^C} \beta_{S^C} \\
&= X_S'( \beta_S ' + \gamma_{SS^C} \beta_{S^C} )
\end{align}
Thus, $\beta_S" = \beta_S' + \gamma_{SS^C}\beta_{S^C}$.

We therefore want to show that $\|\beta_S'\|_{1,2} + \|\beta_{S^C}\|_{1,2} \geq \| \beta_{S.}' + \gamma_{SS^C}\beta_{S^C}\|_{1,2}$  % this seems
or possibly (applying subadditivity)
$\| \gamma_{SS^C}\beta_{S^C}\|_{1,2} \leq \|\beta_{S^C}\|_{1,2}$ % NOTE (Sam): this doesn't seem true in cases where the previous inequality holds
We know that $\| \gamma_{Sp}\|\leq 1$.

%Expand 
%\begin{align}
%\| \gamma_{SS^C}\beta_{S^C}\|_{1,2} &= \sum_{p \in S} \| \gamma_{pS^C} \beta_{S^C}\|_2 \\
%&= \sum_{p \in S} \| \sum_{p' \in S^C} \gamma_{pp'} \beta_{p'}\|_2 \\
%&= \sum_{p \in S} (\sum_{d = 1}^D (\sum_{p' \in S^C} \gamma_{pp'} \beta_{p'd})^2)^{1/2} \\
%&= \sum_{p \in S} (\sum_{d = 1}^D (\sum_{p' \in S^C} (\sum_{p" \in S^C} \gamma_{pp'} \beta_{p'd}\gamma_{pp"} \beta_{p"d} )))^{1/2} \\
%&= \sum_{p \in S} (\sum_{p' \in S^C} ( \sum_{p" \in S^C}( \sum_{d = 1}^D \gamma_{pp'} \beta_{p'd}\gamma_{pp"} \beta_{p"d}) ))^{1/2} \\
%&= \sum_{p \in S} (\sum_{d = 1}^D (\gamma_{p.} \odot \beta_{.d} ) \otimes (\gamma_{p.} \odot \beta_{.d} ))^{1/2} 
%\end{align}

%Meanwhile,
%\begin{align}
%\|\beta_{S^C}\|_{1,2} &= \sum_{p \in S^C} \|\beta_{p}\|_2 \\
%&= \sum_{p \in S^C} (\sum_{d = 1}^D \beta_{pd}^2 )^{1/2}
%\end{align}
%Note that $\beta_{S^C} \in \mathbb R^{(P-D) \times D}$.
% NOTE (Sam): is GL norm submultiplicative for rectangular matrices?  I don't thinkthis would help but interesting question.


%Another way to say this is $ \sum ( \|\beta_p' + \gamma_{pc} \beta_c \|_2  -  \|\beta_p'\|_2) \leq \|\beta_{c.}\| $
% NOTE (Sam): this isn't a tight enough bound since 1 norm is greater than 2 norm expect on axes.
%As in the proof of Proposition \ref{prop:support_orthogonalization}, $X_{.S'}$ unitary and $\|X_{.c}\|_2 \leq 1$ together imply that $\|\gamma_{Sc}\|_2 \leq 1$. 
%Moreover, Proposition \ref{prop:subadditivity} implies that $ \| \beta_{S'.} + \gamma_{Sc}\beta_{c.}^T\|_{1,2} \leq \| \beta_{S'.} \|_{1,2}+ \| \gamma_{Sc}\beta_{c.}^T\|_{1,2}$.
%Thus, we now want to show that $\| \gamma_{Sc} \beta_{c.}^T\|_{1,2} \leq \|\beta_{c.}\|_2$.
%Examining each row, we have that $\| \gamma_{Sc} \beta_{c.}^T\|_{1,2} = \sum_{p \in S} \| \gamma_{pc} \beta_c \|_2 =  \sum_{p \in S} | \gamma_{pc}| \| \beta_c \|_2$


% NOTE (Sam): in my hands its not clear that submultiplicativity gives a tight enough bound.
% In particular, we show that $\| \gamma_{Sc} \beta_{c.}^T\|_{1,2} \leq \| \gamma_{Sc} \|_{2} \|\beta_{c.}\|_{2} $.
%We rewrite $\gamma_{Sc} \beta_{c.}^T = [\gamma_{Sc}] \text{diag}(\beta_{c.})$.
%By submultiplicativity of $\|\|_{1,2}$ we have 

%Then
%\begin{align}
%\| \gamma_{Sc}\beta_{c.}^T\|_{1,2}  &= \sum_{p\in S} \|\gamma_{pc} \beta_{c.}\|_2 \\
%&=  \sum_{p\in S} |\gamma_{pc}| \| \beta_{c.}\|_2 
%\end{align}

%Since $\|\gamma_{Sc}\| \leq 1$, $\sum_{p \in S} \gamma_{pc} \|\beta_p\| \leq \|$
\end{proof}
\end{proof}

\end{proof}
\subsubsection{Proof of Proposition \ref{prop:lasso_selection_equivalence} }

This proof proceeds similarly to the proof of Proposition  \ref{prop:basis_pursuit_selection_equivalence} .
Proposition \ref{prop:lasso_selection_equivalence} relies on the fact that its loss is invariant under any unitary transformation.
As a corollary, this fact gives that the identify matrix which is the "dependent variable" in the regression equation may be replaced by any $d \times d$ unitary matrix.
For Proposition \ref{prop:basis_pursuit_selection_equivalence}, the loss is also invariant under unitary, transformation, but we also check that this transformation.
Once again, this also implies that any unitary matrix may replace the identity in the constraint.
%Minimizer?

 \begin{proposition}{Loss equivalence}
 \label{prop:lasso_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $l_\lambda (\mathcal X, \beta) = l_\lambda (U \mathcal X, \beta U)$.
\end{proposition}

\begin{proof}
We can write 
\begin{eqnarray}
l^*(X^i) = l(\beta^i) = \sum_{j = 1}^p (\sum_{i'=2}^n \| \beta_{i'j.} \|_2^2 +  \|  \beta_{1j.}^i \|_2^2 )^{1/2}=  \sum_{j = 1}^p (\sum_{i'=1}^n \| \beta_{i'j.} U \|_2^2)^{1/2} = l^*(X)
\end{eqnarray}
where the second to last equality is because the norm $\|v\|_2^2 $ is unitary invariant.

\end{proof}

\begin{proposition}{Programmatic equivalence}
\label{prop:lasso_program_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\hat \beta_{\lambda}  (U \mathcal X) = U\hat \beta_{\lambda} (\mathcal X)$.
\end{proposition}



\subsection{Experiments}


