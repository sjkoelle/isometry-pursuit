\documentclass[a4paper,11pt]{article}
\input{config.tex}

\begin{document}

%This note specifically targets the problem of local isometry estimation using a convex Tangent Space Lasso-type method.
%It does not directly reproduce the argument that tangent space basis pursuit selects for a "most isometric embedding" from my thesis, nor would I expect this to go in the same paper as tangent space lasso.
%Other interesting open questions about basis pursuit that are not covered here include can we equate the intrinsic curvature of the data manifold to the basis pursuit loss, or what happens when dictionary = feature space, or what happens if we estimate in the coordinates of the ambient space, or what are the convergence rates.
%I chose to focus on local isometry estimation because it seemed distinct enough from what everyone else was working on that it could be published as a workshop paper on its own, and because the mathematical components are a little novel, but not disorientingly so, and because it solves an open problem noticed by another research group.
\title{Sparse isometry pursuit}
\author{Samson Koelle, Marina Meila}

\maketitle

\begin{abstract}

Sparse isometry pursuit is an algorithm for identifying unitary column-submatrices in polynomial time.
It achieves sparsity via use of the group lasso norm, and therefore has constrained basis pursuit and penalized group lasso formulations.
Applied to tabular data, it selects a subset of columns that maximize diversity.
Applied to Jacobians of putative coordinate functions, it is a useful subroutine for identifying isometric embeddings from within dictionaries and therefore has relevance to interpretability of learned representations.

% singular value criteria could be directly optimized.  singular value criteria would not be sparse
% recommendation systems.  
% RAG

\end{abstract}

\section{Introduction}

%Although computing the domain of a wide rectangular matrix is among the most classic problems in optimization and machine learning, the related problem of selection of the column-subset that forms the best basis for this domain is relatively unstudied.
%However, 

Many real-life problems may be abstracted as selecting a subset of the columns of a matrix.
Matrices representing statistical observations or analytically exact data like gradients arise in many applications.
Column-selection then corresponds to the problem of which variables or functions of the data to retain. %(RAG, recommendations, training neural networks (infrequent meta), wind).
The efficient and principled selection of variables is therefore a critical and popular area of research.

This paper focuses on unsupervised variable selection.
%Depending on the problem, the method of selection may differ.
Supervised learning, in which a response variable is predicted by
In contrast, unsupervised variable selection as a type of dimension reduction has received comparably less attention.
Unsupervised learning methods like PCA, UMAP, and autoencoders are often concerned with minimizing reconstruction error without regard for the sparsity of the learned respresentation, and among those that sparsify, variable selection methods contrast with sparsification with respect to reconstruction from a learned latent space.

The core literature for variable selection comes from sparse PCA.

In the context of non-linear dimension reduction, dictionary-based methods, sparse reconstruction exists.
Not to be confused with dictionary-learning methods.
% Although methods like sparse autoencoders, sparse PCA exist 

The problem of computing a sparse inverse matrix. % Something about finding balance.august 

Within the context of non-linear dimension reduction, selection of coordinate functions of an embedding space from within a dictionary is a core problem in geometric data analysis.  % more on interpretability
In order of specificity, these methods may seek to optimize independent coordinates \cite{NEURIPS2019_6a10bbd4, He2023-ch}, low distortion embeddings, or isometric embeddings.
% low distortion is conformal
Dictionaries can be either given \cite{Koelle2022-ju, Koelle2024-no} or learned \cite{Kohli2021-lr}.
Optimization can be global or local.

These coordinate selection algorithms can be greedy \cite{NEURIPS2019_6a10bbd4, Kohli2021-lr} or convex \cite{Koelle2022-ju, Koelle2024-no}.
In this paper we show that an adapted version of the group lasso type algorithm in \cite{Koelle2024-no} leads to a convex procedure competitive with previous greedy approaches with respect to isometry.
This approach relies on a to-our-knowledge novel matrix inversion algorithm that is sparse in the column space of the matrix.
This method displays the favorable characteristics of group lasso type problems, including duality of a regularized form with a basis pursuit problem.
These problems are solvable with off-the-shelf multitask lasso and interior point solvers, respectively.
% cite some classic manifold learning papers

since isometry embeddings preserve important properties like distances between points.
We describe a convex optimization approach for selection such functions based on the Tangent Space Lasso.
This approach combines a strict theoretical criterion and computationally expediency.

% Recommendation, RAG

%Compared with losses such as operator norm, this loss is symmetric accross the entire spectrum

\section{Background}

We are given a rank $D$ matrix $\mathcal X \in \mathbb R^{D \times P}$ with $P > D$.
$X$ could be, for example, the Jacobian matrix $d\mathcal G$ of a set of candidate coordinate functions.
We assume that we are given a target dimension $T \leq D$
% BIG QUESTION: RUN IN D = B?  What is rank of $X$?    It's D if not running D = B

\section{Problem}

Our goal is to select a subset $\mathcal S \subset [P]$ with $|\mathcal S| = D$ such that $X_{. \mathcal S}$ is unitary.

\section{Method}

Define the group basis pursuit penalty norm % is this really a norm?
\begin{align}
\ref{eq:bp}
\|\|_{1,2} \mathbb R^{P \times D} \to \mathbb R^+
\beta \mapsto  \sum_{p=1}^P  \|\beta_{p.}\|_2.
\end{align}

% Direct minimization of this norm has nothing to do with correlation.

Direct minimization of Equation \ref{eq:bp} will not select for isometry due to the preference for columns with larger norm.
Define the transform
\begin{align}
\exp_1: \mathbb R^{D \times P} \to \mathbb R^{D \times P} \\ 
\mathcal X \mapsto \exp (- | \log \|{\mathcal X }_{j.}\|_2)|) \frac{{\mathcal X }_{j.}}{\|{\mathcal X }_{j.}\|}.
\end{align}


%Define the loss function
%\begin{align}
%l (X, \beta) := \|I_d - X\beta\|^2 + \|\beta\|_{2,1}
%\end{align}

We can also define the basis pursuit loss
\begin{align}
m (X, \beta) := \|\beta\|_{2,1} :  I_d = X\beta
\end{align}

Our main interest is in analyzing the properties of $l(\exp_1 X,  \beta)$ and $m(\exp_1 X,  \beta)$ 

This is the main loss function whose properties we analyze.


%One immediate question is - why $\beta$?  Can we just minimize some function of $X$ directly like $\|\exp_1 X\|_{1,2}$?  Well, then we wouldn't need orthogonality - just constant length!

\subsection{Tangent Space Lasso}

The intuition is that vectors which are closer to 1 in length and more orthogonal will be smaller in loss.

\begin{proposition}{Unitary subset selection}
Given a $X$ that contains a unique subset $X^* \in \mathbb R^{d \times d} $ such that $X^*$ is unitary and full rank, then $X^* = \arg \min_\beta l(\exp_1(X),\beta)$.
\end{proposition}


Before proceeding, we require the following piece of Lemma \ref{lemma:rotinv1}. 
 \begin{proposition}
 \label{prop:unitarybasis}
Consider two sets of vector fields $X$ and $X^i$ where $X_{i..}^i = U X_{i..} $, where $U$ is unitary and $X_{i'..} = X_{i'..}^i$ for other values $i' \neq i$.
Then $l^*(X) = l^*(X^i)$
\end{proposition}

\begin{proof}
Without loss of generality, let $i = 1$.
We can write 
\begin{eqnarray}
l^*(X^i) = l(\beta^i) = \sum_{j = 1}^p (\sum_{i'=2}^n \| \beta_{i'j.} \|_2^2 +  \|  \beta_{1j.}^i \|_2^2 )^{1/2}=  \sum_{j = 1}^p (\sum_{i'=1}^n \| \beta_{i'j.} U \|_2^2)^{1/2} = l^*(X)
\end{eqnarray}
where the second to last equality is because the norm $\|v\|_2^2 $ is unitary invariant.
\end{proof}



We first show that vectors which are more orthogonal will be smaller in loss.

\begin{proposition}
\label{lemma:orthogonal}
Let $X_{.S} \in \mathbb R^{d \times p}$ be defined as above and let $X_{..S}'$ be an array such that $\|X_{.S_j}'\|_2 = \|X_{.S_j}\|_2$ for all $j \in [d]$ and $X_{.S}'$ is column-orthogonal.
Then $\tilde l^* (X_{..S}) > \tilde l^* (X_{..S}')$.
\end{proposition}
\begin{proof}

By Lemma \ref{prop:unitarybasis}, without loss of generality
\begin{eqnarray}
\beta_{ijk}^i = \begin{cases} \|\tilde X_{.S_j}'\|_2^{-1} & j = k \in \{ 1 \dots d\}  \\
0 & \text{otherwise}
\end{cases}.
\end{eqnarray}
Therefore,
\begin{eqnarray}
\tilde l^*(X') = \sum_{j = 1}^d \sqrt{\sum_{i = 1}^n \|\tilde X_{i.S_j}' \|_2^{-2}}.
\end{eqnarray}

On the other hand, the invertible matrices $\tilde X_{.S}$ admit QR decompositions $\tilde X_{.S} = QR$ where $Q$ and $R$ are square unitary and upper-triangular matrices, respectively \cite{Anderson1992-fb}.
Since $l^*$ is invariant to unitary transformations, we can without loss of generality, consider $Q= I_d$.
Denoting $I_d$ to be composed of basis vectors $[e^1 \dots e^d]$, the matrix $R$ has form
\begin{eqnarray}
R = \begin{bmatrix}
\langle e^1, \tilde X_{i.{S_1}} \rangle & \langle e^1, \tilde X_{i.{S_2}} \rangle  &\dots &  \langle e^1, \tilde X_{i.{S_d}} \rangle \\
0 & \langle e^2, \tilde X_{i.{S_2}} \rangle & \dots  &  \langle e^2, \tilde X_{i.{S_d}} \rangle\\
0 & 0 & \dots & \dots  \\
0 & 0 & 0& \langle e^d, \tilde X_{i.{S_d}} \rangle 
\end{bmatrix}.
\end{eqnarray}
The diagonal entries $R_{jj} = \langle q^j, \tilde X_{.{S_j}} \rangle$ of this matrix have form $\| \tilde X_{.{S_j}} -  \sum_{j' \in \{1 \dots j-1\}} \langle \tilde X_{.{S_{j}}}, e^{j'} \rangle e^{j'} \|$.
Thus, $R_{j} \in (0, \| \tilde X_{i.{S_j}}\|]$.
On the other hand $\beta_{iS.} =R^{-1}$, which has diagonal elements $\beta_{j} = R_{j}^{-1}$, since $R$ is upper triangular.
Thus, $\beta_{jj} \geq \| \tilde X_{.{S_j}}\|^{-1}$, and therefore $\|\beta_{iS_j.}\| \geq \|\beta_{S_j.}'\|.$
Since $\|\beta_{S_j.}\| \geq \|\beta_{S_j.}'\|$ for all $i$, then $\|\beta_{.S_j.}\| \geq \|\beta_{.S_j.}' \|$.
%Finally, that this is true for all $j$.
%That is, if we 
\end{proof}

The above proposition formalizes our intuition that orthogonality of $X$ lowers $l^*(X)$ over non-orthogonality.
We now show a similar result for the somewhat less intuitive heuristic that dictionary functions whose gradient fields are length $1$ will be favored over those which are non-constant.
Since the result on orthogonality holds regardless of length, we need only consider the case where the component vectors in our sets of vector fields are mutually orthogonal at each data point, but not necessarily of norm $1$.
Note that were they not orthogonal, making them so would also reduce $l^*$.
We then show that vectors which are closer to length $1$ are lower in loss.
Since vectors which are closer to length $1$ are shrunk in length less by $\exp_1$, their corresponding loadings are smaller.
This is formalized in the following proposition


\begin{proposition}
\label{lemma:orthogonal}
Let $X_{.S}^{''}$ be a set of vector fields $X_{.S_j}^{''}$ mutually orthogonal at every data point $i$, and $\|X_{.S_j}^{''}\| = 1$.
Then $\tilde l^* (X_{.S}' ) \geq \tilde l^*(X_{.S}^{''})$.
\end{proposition}
\begin{proof}
Let $\|X_{i.S_j}^{''}\| = c_j$.  By Proposition \ref{prop:unitarybasis}, we can assume without loss of generality (i.e without changing the loss) that
\begin{eqnarray}
\tilde X_{.S_j} = \begin{bmatrix}
c_1 & 0 & 0 \\
0 & \dots & 0  \\
0 & 0 & c_d  \rangle\\
\end{bmatrix}.
\end{eqnarray}
Thus
\begin{eqnarray}
\tilde \exp_1 X_{.S_j} = \begin{bmatrix}
 \exp (- | \log \|c_1 \|_2)|)  & 0 & 0 \\
0 & \dots & 0  \\
0 & 0 &  \exp (- | \log \|c_d \|_2)|)   \rangle\\
\end{bmatrix}.
\end{eqnarray}
and therefore
\begin{eqnarray}
\tilde \beta_{.S_j} = \begin{bmatrix}
 \exp (- | \log \|c_1 \|_2)|)^{-1}  & 0 & 0 \\
0 & \dots & 0  \\
0 & 0 &  \exp (- | \log \|c_d \|_2)|)^{-1}   \rangle\\
\end{bmatrix}.
\end{eqnarray}
The question is therefore what values of $c_j$ minimize $\exp (- | \log \ |c_1 \|_2)|)^{-1} $.  $| \log \ |c_1 \|_2)|$ is minimized (evaluates to $0$) when $c_j = 1$, so $- | \log \ |c_1 \|_2)|$ is maximized (evaluates to $0$, so $\exp (- | \log \ |c_1 \|_2)|)$ is maximized (evaluates to $1$), so $\exp (- | \log \ |c_1 \|_2)|)^{-1}$ is minimized (evaluates to $1$).
\end{proof}

\begin{proposition}{Local Isometry}
Given a set of functions $G$ that contains a subset that defines a locally isometric embedding at a point $\xi$, then these will be selected as $\arg \min_\beta$.
\end{proposition}

Algorithm (Local tangent Space basis pursuit)

Algorithm (Local two stage tangent space basis pursuit)

This provides an approach for the problem put forward in (cite) LDLE paper.

Experiments (Loss)

Compare with isometry loss (2 norm of singular values).

\section{Experiments}

Comparison with isometry loss.

%The Hoeffding bound
%Dimension estimation, the failure of duality
%The presence of curvature

\section{Discussion}

It could be used in the stiching step of an algorithm like the kohli one
We leave aside the question of patch alignment \cite{https://arxiv.org/pdf/2303.11620.pdf, LDLE paper}.
The full gradient approach.
In this case normalization prior to projection is subsumbmed by the larger coefficients needed to get the tangent space.
Good news is tangent space estimation need not be performed.
Let's compare the coefficients involved in projecting versus not projecting.
We can perform regression in the high dimensional space instead of projecting on span of target variable.


\section{Supplement}

% Proof of isometry (Copy from thesis)

Proof of local isometry (simpler proof since no oscillation game)

%\section{Comparison with other approach}


%\subsection{Projection approach}

%\begin{align}
%x_M = (y^T y) x \\ 
%({x_M}^T {x_M})^{-1} {x_M} I = ({(y^T y x)}^T {(y^T y x)})^{-1} {(y^T y x)} I = (x^T y^T y x)^{-1} (y^T y x)
%\end{align}

%low distortion just means conformal


\end{document}