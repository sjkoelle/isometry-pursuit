\documentclass[a4paper,11pt]{article}
\input{config.tex}

\begin{document}

%This note specifically targets the problem of local isometry estimation using a convex Tangent Space Lasso-type method.
%It does not directly reproduce the argument that tangent space basis pursuit selects for a "most isometric embedding" from my thesis, nor would I expect this to go in the same paper as tangent space lasso.
%Other interesting open questions about basis pursuit that are not covered here include can we equate the intrinsic curvature of the data manifold to the basis pursuit loss, or what happens when dictionary = feature space, or what happens if we estimate in the coordinates of the ambient space, or what are the convergence rates.
%I chose to focus on local isometry estimation because it seemed distinct enough from what everyone else was working on that it could be published as a workshop paper on its own, and because the mathematical components are a little novel, but not disorientingly so, and because it solves an open problem noticed by another research group.
\title{Sparse isometry pursuit}
\author{Samson Koelle, Marina Meila}

\maketitle

\begin{abstract}

Sparse isometry pursuit is an algorithm for identifying unitary column-submatrices in polynomial time.
It achieves sparsity via use of the group lasso norm, and therefore has constrained basis pursuit and penalized group lasso formulations.
Applied to tabular data, it selects a subset of columns that maximize diversity.
Applied to Jacobians of putative coordinate functions, it is a useful subroutine for identifying isometric embeddings from within dictionaries and therefore has relevance to interpretability of learned representations.

% singular value criteria could be directly optimized.  singular value criteria would not be sparse
% recommendation systems.  
% RAG

\end{abstract}

\section{Introduction}

%Although computing the domain of a wide rectangular matrix is among the most classic problems in optimization and machine learning, the related problem of selection of the column-subset that forms the best basis for this domain is relatively unstudied.
%However, 

Many real-life problems may be abstracted as selecting a subset of the columns of a matrix.
Matrices representing statistical observations or analytically exact data like gradients arise in many applications.
Column-selection then corresponds to the problem of which variables or functions of the data to retain. %(RAG, recommendations, training neural networks (infrequent meta), wind).
The efficient and principled selection of variables is therefore a critical and popular area of research.

This paper focuses on unsupervised variable selection.
%Depending on the problem, the method of selection may differ.
Supervised learning, in which a response variable is predicted by
In contrast, unsupervised variable selection as a type of dimension reduction has received comparably less attention.
Unsupervised learning methods like PCA, UMAP, and autoencoders are often concerned with minimizing reconstruction error without regard for the sparsity of the learned respresentation, and among those that sparsify, variable selection methods contrast with sparsification with respect to reconstruction from a learned latent space.

. % prob should compare with this

In the context of non-linear dimension reduction, dictionary-based methods, sparse reconstruction exists.
Not to be confused with dictionary-learning methods.
% Although methods like sparse autoencoders, sparse PCA exist 

Multitask learning and lasso \cite{Hastie2015-qa}

Within the context of non-linear dimension reduction, selection of coordinate functions of an embedding space from within a dictionary is a core problem in geometric data analysis.  % more on interpretability
In order of specificity, these methods may seek to optimize independent coordinates \cite{Chen2019-km, He2023-ch}, low distortion embeddings, or isometric embeddings.
% low distortion is conformal
Dictionaries can be either given \cite{Koelle2022-ju, Koelle2024-no} or learned \cite{Kohli2021-lr}.
Optimization can be global or local.

These coordinate selection algorithms can be greedy \cite{NEURIPS2019_6a10bbd4, Kohli2021-lr, Jones2007-uc} or convex \cite{Koelle2022-ju, Koelle2024-no}.
In this paper we show that an adapted version of the group lasso type algorithm in \cite{Koelle2024-no} leads to a convex procedure competitive with previous greedy approaches with respect to isometry.
This approach relies on a to-our-knowledge novel matrix inversion algorithm that is sparse in the column space of the matrix.
This method displays the favorable characteristics of group lasso type problems, including duality of a regularized form with a basis pursuit problem.
These problems are solvable with off-the-shelf multitask lasso and interior point solvers, respectively.
% cite some classic manifold learning papers

The comparison of greedy (e.g. Orthogonal Matching Pursuit) \cite{Mallat93-wi, Tropp05-ml} and convex \cite{Tropp06-sg} basis pursuit formulations.


Compared with sparse pca \cite{Bertsimas2022-qo, Bertsimas2022-dv}, we are not concerned with variability in the dataset, and select.
The core literature for variable selection comes from sparse PCA \cite{Dey2017-mx}
While the sparse PCA problem is non-convex, our approach can be taken as a simpler version in the sense that the loadings are constrained to be the identify matrix.


\cite{Tropp06-sg} and \cite{Liu2009-yo} use a $1,\infty$ norm to induce sparsity that misses the utility of our normalization for finding unitary matrices.
since isometry embeddings preserve important properties like distances between points.
We describe a convex optimization approach for selection such functions based on the Tangent Space Lasso.
This approach combines a strict theoretical criterion and computationally expediency.

Basis pursuit \cite{Chen2001-hh} 
% Recommendation, RAG

Compared with losses such as operator norm, this loss is symmetric across the entire spectrum.  Compared with the nuclear norm, it is symmetric around 1 \cite{Fazel2001ARM}. % is our loss self-dual?
% compared with sparse pca loadings are constrained to be 1.  is it convex?

\section{Background}

We are given a rank $D$ matrix $\mathcal X \in \mathbb R^{D \times P}$ with $P > D$.
$X$ could be, for example, the Jacobian matrix $d\mathcal G$ of a set of candidate coordinate functions.
We assume that we are given a target dimension $T \leq D$
% BIG QUESTION: RUN IN D = B?  What is rank of $X$?    It's D if not running D = B

\section{Problem}

Our goal is to select a subset $\mathcal S \subset [P]$ with $|\mathcal S| = D$ such that $X_{. \mathcal S}$ is unitary.

\section{Method}

Define the group basis pursuit penalty norm % is this really a norm?
\begin{align}
\ref{eq:bp}
\|\|_{1,2} \mathbb R^{P \times D} \to \mathbb R^+
\beta \mapsto  \sum_{p=1}^P  \|\beta_{p.}\|_2.
\end{align}

% Direct minimization of this norm has nothing to do with correlation.

Direct minimization of Equation \ref{eq:bp} will not select for isometry due to the preference for columns with larger norm.
Define the transform
\begin{align}
\exp_1: \mathbb R^{D \times P} \to \mathbb R^{D \times P} \\ 
\mathcal X \mapsto \exp (- | \log \|{\mathcal X }_{j.}\|_2)|) \frac{{\mathcal X }_{j.}}{\|{\mathcal X }_{j.}\|}.
\end{align}


%Define the loss function
%\begin{align}
%l (X, \beta) := \|I_d - X\beta\|^2 + \|\beta\|_{2,1}
%\end{align}

We can also define the basis pursuit loss
\begin{align}
m (X, \beta) := \|\beta\|_{2,1} :  I_d = X\beta
\end{align}

Our main interest is in analyzing the properties of $l(\exp_1 X,  \beta)$ and $m(\exp_1 X,  \beta)$ 

This is the main loss function whose properties we analyze.


%One immediate question is - why $\beta$?  Can we just minimize some function of $X$ directly like $\|\exp_1 X\|_{1,2}$?  Well, then we wouldn't need orthogonality - just constant length!

\subsection{Tangent Space Lasso}

The intuition is that vectors which are closer to 1 in length and more orthogonal will be smaller in loss.

\begin{proposition}{Unitary subset selection}
Given a $X$ that contains a unique subset $X^* \in \mathbb R^{d \times d} $ such that $X^*$ is unitary and full rank, then $X^* = \arg \min_\beta l(\exp_1(X),\beta)$.
\end{proposition}


Before proceeding, we require the following piece of Lemma \ref{lemma:rotinv1}. 
 \begin{proposition}
 \label{prop:unitarybasis}
Consider two sets of vector fields $X$ and $X^i$ where $X_{i..}^i = U X_{i..} $, where $U$ is unitary and $X_{i'..} = X_{i'..}^i$ for other values $i' \neq i$.
Then $l^*(X) = l^*(X^i)$
\end{proposition}

\begin{proof}
Without loss of generality, let $i = 1$.
We can write 
\begin{eqnarray}
l^*(X^i) = l(\beta^i) = \sum_{j = 1}^p (\sum_{i'=2}^n \| \beta_{i'j.} \|_2^2 +  \|  \beta_{1j.}^i \|_2^2 )^{1/2}=  \sum_{j = 1}^p (\sum_{i'=1}^n \| \beta_{i'j.} U \|_2^2)^{1/2} = l^*(X)
\end{eqnarray}
where the second to last equality is because the norm $\|v\|_2^2 $ is unitary invariant.
\end{proof}



We first show that vectors which are more orthogonal will be smaller in loss.

\begin{proposition}
\label{lemma:orthogonal}
Let $X_{.S} \in \mathbb R^{d \times p}$ be defined as above and let $X_{..S}'$ be an array such that $\|X_{.S_j}'\|_2 = \|X_{.S_j}\|_2$ for all $j \in [d]$ and $X_{.S}'$ is column-orthogonal.
Then $\tilde l^* (X_{..S}) > \tilde l^* (X_{..S}')$.
\end{proposition}
\begin{proof}

By Lemma \ref{prop:unitarybasis}, without loss of generality
\begin{eqnarray}
\beta_{ijk}^i = \begin{cases} \|\tilde X_{.S_j}'\|_2^{-1} & j = k \in \{ 1 \dots d\}  \\
0 & \text{otherwise}
\end{cases}.
\end{eqnarray}
Therefore,
\begin{eqnarray}
\tilde l^*(X') = \sum_{j = 1}^d \sqrt{\sum_{i = 1}^n \|\tilde X_{i.S_j}' \|_2^{-2}}.
\end{eqnarray}

On the other hand, the invertible matrices $\tilde X_{.S}$ admit QR decompositions $\tilde X_{.S} = QR$ where $Q$ and $R$ are square unitary and upper-triangular matrices, respectively \cite{Anderson1992-fb}.
Since $l^*$ is invariant to unitary transformations, we can without loss of generality, consider $Q= I_d$.
Denoting $I_d$ to be composed of basis vectors $[e^1 \dots e^d]$, the matrix $R$ has form
\begin{eqnarray}
R = \begin{bmatrix}
\langle e^1, \tilde X_{i.{S_1}} \rangle & \langle e^1, \tilde X_{i.{S_2}} \rangle  &\dots &  \langle e^1, \tilde X_{i.{S_d}} \rangle \\
0 & \langle e^2, \tilde X_{i.{S_2}} \rangle & \dots  &  \langle e^2, \tilde X_{i.{S_d}} \rangle\\
0 & 0 & \dots & \dots  \\
0 & 0 & 0& \langle e^d, \tilde X_{i.{S_d}} \rangle 
\end{bmatrix}.
\end{eqnarray}
The diagonal entries $R_{jj} = \langle q^j, \tilde X_{.{S_j}} \rangle$ of this matrix have form $\| \tilde X_{.{S_j}} -  \sum_{j' \in \{1 \dots j-1\}} \langle \tilde X_{.{S_{j}}}, e^{j'} \rangle e^{j'} \|$.
Thus, $R_{j} \in (0, \| \tilde X_{i.{S_j}}\|]$.
On the other hand $\beta_{iS.} =R^{-1}$, which has diagonal elements $\beta_{j} = R_{j}^{-1}$, since $R$ is upper triangular.
Thus, $\beta_{jj} \geq \| \tilde X_{.{S_j}}\|^{-1}$, and therefore $\|\beta_{iS_j.}\| \geq \|\beta_{S_j.}'\|.$
Since $\|\beta_{S_j.}\| \geq \|\beta_{S_j.}'\|$ for all $i$, then $\|\beta_{.S_j.}\| \geq \|\beta_{.S_j.}' \|$.
%Finally, that this is true for all $j$.
%That is, if we 
\end{proof}

The above proposition formalizes our intuition that orthogonality of $X$ lowers $l^*(X)$ over non-orthogonality.
We now show a similar result for the somewhat less intuitive heuristic that dictionary functions whose gradient fields are length $1$ will be favored over those which are non-constant.
Since the result on orthogonality holds regardless of length, we need only consider the case where the component vectors in our sets of vector fields are mutually orthogonal at each data point, but not necessarily of norm $1$.
Note that were they not orthogonal, making them so would also reduce $l^*$.
We then show that vectors which are closer to length $1$ are lower in loss.
Since vectors which are closer to length $1$ are shrunk in length less by $\exp_1$, their corresponding loadings are smaller.
This is formalized in the following proposition


\begin{proposition}
\label{lemma:orthogonal}
Let $X_{.S}^{''}$ be a set of vector fields $X_{.S_j}^{''}$ mutually orthogonal at every data point $i$, and $\|X_{.S_j}^{''}\| = 1$.
Then $\tilde l^* (X_{.S}' ) \geq \tilde l^*(X_{.S}^{''})$.
\end{proposition}
\begin{proof}
Let $\|X_{i.S_j}^{''}\| = c_j$.  By Proposition \ref{prop:unitarybasis}, we can assume without loss of generality (i.e without changing the loss) that
\begin{eqnarray}
\tilde X_{.S_j} = \begin{bmatrix}
c_1 & 0 & 0 \\
0 & \dots & 0  \\
0 & 0 & c_d  \rangle\\
\end{bmatrix}.
\end{eqnarray}
Thus
\begin{eqnarray}
\tilde \exp_1 X_{.S_j} = \begin{bmatrix}
 \exp (- | \log \|c_1 \|_2)|)  & 0 & 0 \\
0 & \dots & 0  \\
0 & 0 &  \exp (- | \log \|c_d \|_2)|)   \rangle\\
\end{bmatrix}.
\end{eqnarray}
and therefore
\begin{eqnarray}
\tilde \beta_{.S_j} = \begin{bmatrix}
 \exp (- | \log \|c_1 \|_2)|)^{-1}  & 0 & 0 \\
0 & \dots & 0  \\
0 & 0 &  \exp (- | \log \|c_d \|_2)|)^{-1}   \rangle\\
\end{bmatrix}.
\end{eqnarray}
The question is therefore what values of $c_j$ minimize $\exp (- | \log \ |c_1 \|_2)|)^{-1} $.  $| \log \ |c_1 \|_2)|$ is minimized (evaluates to $0$) when $c_j = 1$, so $- | \log \ |c_1 \|_2)|$ is maximized (evaluates to $0$, so $\exp (- | \log \ |c_1 \|_2)|)$ is maximized (evaluates to $1$), so $\exp (- | \log \ |c_1 \|_2)|)^{-1}$ is minimized (evaluates to $1$).
\end{proof}

\begin{proposition}{Local Isometry}
Given a set of functions $G$ that contains a subset that defines a locally isometric embedding at a point $\xi$, then these will be selected as $\arg \min_\beta$.
\end{proposition}

Algorithm (Local tangent Space basis pursuit)

Algorithm (Local two stage tangent space basis pursuit)

This provides an approach for the problem put forward in (cite) LDLE paper.

Experiments (Loss)

Compare with isometry loss (2 norm of singular values).

\subsection{Implementation}

We use the multitask lasso from sklearn and the cvxpy package for basis pursuit.  We use the SCS interior point solver from CVXPY, which is able to push sparse values arbitrarily close to 0 \cite{cvxpy_sparse_solution}. Data is IRIS and Wine, as well as flat torus from ldle.
\subsection{Computational complexity}
\section{Experiments}

Comparison with isometry loss.

%The Hoeffding bound
%Dimension estimation, the failure of duality
%The presence of curvature

\section{Discussion}

It could be used in the stiching step of an algorithm like the kohli one
We leave aside the question of patch alignment \cite{https://arxiv.org/pdf/2303.11620.pdf, LDLE paper}.
The full gradient approach.
In this case normalization prior to projection is subsumbmed by the larger coefficients needed to get the tangent space.
Good news is tangent space estimation need not be performed.
Let's compare the coefficients involved in projecting versus not projecting.
We can perform regression in the high dimensional space instead of projecting on span of target variable.

With respect to pseudoinverse estimation, sparse methods have been applied in \cite{Sun2012-vp}

Even though by Lagrangian duality, the basis pursuit solution corresponds to $\lambda approaching $0$, the solution is sparse \cite{Tropp04-ju}.
about the lasso is that all coefficients enter the regularization path.
As we see by the correspondence between $\lambda$ approaching $0$ and the basis pursuit problem, some coefficients in fact do not go to $0$. 
\section{Supplement}

% Proof of isometry (Copy from thesis)

Proof of local isometry (simpler proof since no oscillation game)

% failure of duality

\cite{Bertsimas2022-qo} gives a method for solving the sparse-PCA method more efficiently than the original greedy approach.
Compared with the FISTA method used in  \cite{Koelle2022-ju, Koelle2024-no}, coordinate descent \cite{Friedman-2007-yb, Meier2008-ts, Qin2013-tx} is faster \cite{Catalina2018-ek, Zhao2023-xn}.
Compared with \cite{Liu2009-yo}, the sklearn multitask lasso is $2,1$ rather than $\infty,1$ regularized.

Compared with Gram-Schmidt
It is likely that the transformed singular value loss could be reframed as a semdefinite programming problem, since the composition of two convex functions is convex \cite{Boyd2004-ql}.
%\section{Comparison with other approach}

Multitask lasso \cite{Obozinski2006-kq, Yeung2011-fg} is a form of group lasso \cite{Yuan2006-bt} where coefficients are group by response variable.

See \cite{Obozinski2006-kq} for a comparison of forward and backward selection with lasso.
%\subsection{Projection approach}

Our notion of isometric recovery is distinct from the restricted isometry property \cite{Candes2005-dd, Hastie2015-qa}, which is used to show guaranteed recovery at fast convergence rates in supervised learning.
In particular, our approach does not consider statistical error or the presence of a true underlying model.
However, we note that disintigration of performance at high $\lambda$ values in the lasso formulation may have some relation to these properties, as discussed in \cite{Koelle2022-ju, Koelle2024-no}.
%\begin{align}
%x_M = (y^T y) x \\ 
%({x_M}^T {x_M})^{-1} {x_M} I = ({(y^T y x)}^T {(y^T y x)})^{-1} {(y^T y x)} I = (x^T y^T y x)^{-1} (y^T y x)
%\end{align}

%low distortion just means conformal

A major area of comparison is in diversification in recommendation systems.  Greedy algorithms are used \cite{Carbonell2017-gi, Wu2019-uk}

\end{document}