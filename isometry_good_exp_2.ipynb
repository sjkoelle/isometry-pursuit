{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b813b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import montlake\n",
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ceb7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package montlake:\n",
      "\n",
      "NAME\n",
      "    montlake\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    None\n",
      "    _nbdev\n",
      "    atomgeom (package)\n",
      "    core\n",
      "    exec (package)\n",
      "    geometry (package)\n",
      "    gradients (package)\n",
      "    mflasso (package)\n",
      "    optimization (package)\n",
      "    plotting (package)\n",
      "    simulations (package)\n",
      "    statistics (package)\n",
      "    tslasso (package)\n",
      "    utils (package)\n",
      "    vendor (package)\n",
      "\n",
      "VERSION\n",
      "    0.0.1\n",
      "\n",
      "FILE\n",
      "    /Users/samsonkoelle/miniconda3/envs/montlake_xxx/lib/python3.6/site-packages/montlake/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(montlake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c908161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/optimization.gradientgrouplasso.ipynb (unless otherwise specified).\n",
    "\n",
    "__all__ = ['GradientGroupLasso', 'get_sr_lambda_parallel', 'batch_stream']\n",
    "\n",
    "# Cell\n",
    "#loosely inspired by the pyglmnet package\n",
    "from einops import rearrange\n",
    "#import autograd.numpy as np\n",
    "import numpy as np\n",
    "\n",
    "class GradientGroupLasso:\n",
    "\n",
    "    def __init__(self, dg_M, df_M, reg_l1s, reg_l2, max_iter,learning_rate, tol, beta0_npm= None):\n",
    "\n",
    "        n = dg_M.shape[0]\n",
    "        d= dg_M.shape[1]\n",
    "        m = df_M.shape[2]\n",
    "        p = dg_M.shape[2]\n",
    "        dummy_beta = np.ones((n,p,m))\n",
    "\n",
    "        self.dg_M = dg_M\n",
    "        self.df_M = df_M\n",
    "        self.reg_l1s = reg_l1s\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.beta0_npm = beta0_npm\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.m = m\n",
    "        self.d = d\n",
    "        self.dummy_beta = dummy_beta\n",
    "        #self.group = np.asarray(group)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "\n",
    "    def _prox(self,beta_npm, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "\n",
    "        p = self.p\n",
    "        result = np.zeros(beta_npm.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        for j in range(p):\n",
    "            if np.linalg.norm(beta_npm[:,j,:]) > 0.:\n",
    "                potentialoutput = beta_npm[:,j,:] - (thresh / np.linalg.norm(beta_npm[:,j,:])) * beta_npm[:,j,:]\n",
    "                posind = np.asarray(np.where(beta_npm[:,j,:] > 0.))\n",
    "                negind = np.asarray(np.where(beta_npm[:,j,:] < 0.))\n",
    "                po = beta_npm[:,j,:].copy()\n",
    "                po[posind[0],posind[1]] = np.asarray(np.clip(potentialoutput[posind[0],posind[1]],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind[0],negind[1]] = np.asarray(np.clip(potentialoutput[negind[0],negind[1]],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[:,j,:] = po\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta_npm):\n",
    "\n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "        reg_l2 = self.reg_l2\n",
    "        dummy_beta = self.dummy_beta\n",
    "\n",
    "        df_M_hat = np.einsum('ndp,npm->ndm',dg_M, beta_npm)\n",
    "        error = df_M_hat - df_M\n",
    "        grad_beta = np.einsum('ndm,ndp->npm',error,dg_M) #+ reg_l2 * np.ones()\n",
    "        #if\n",
    "        return grad_beta\n",
    "\n",
    "    def _L1penalty(self, beta_npm):\n",
    "\n",
    "        p = self.p\n",
    "        m = self.m\n",
    "        n = self.n\n",
    "        beta_mn_p = rearrange(beta_npm, 'n p m -> (m n) p')#np.reshape(beta_mnp, ((m*n,p)))\n",
    "        L1penalty = np.linalg.norm(beta_mn_p, axis = 0).sum()\n",
    "\n",
    "        return L1penalty\n",
    "\n",
    "    def _loss(self,beta_npm, reg_lambda):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta_npm)\n",
    "        P = self._L1penalty(beta_npm)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "\n",
    "    def _logL(self,beta_npm):\n",
    "\n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "\n",
    "        df_M_hat = np.einsum('ndp,npm -> ndm',dg_M, beta_npm)\n",
    "        logL = -0.5 * np.linalg.norm((df_M - df_M_hat))**2\n",
    "        return(logL)\n",
    "\n",
    "    def _L2loss(self,beta_npm):\n",
    "        output = -self._logL(beta_npm)\n",
    "        return(output)\n",
    "\n",
    "    def fhatlambda(self,learning_rate,beta_npm_new,beta_npm_old):\n",
    "\n",
    "        #print('lr',learning_rate)\n",
    "        output = self._L2loss(beta_npm_old) + np.einsum('npm,npm', self._grad_L2loss(beta_npm_old),(beta_npm_new-beta_npm_old)) + (1/(2*learning_rate)) * np.linalg.norm(beta_npm_new-beta_npm_old)**2\n",
    "\n",
    "        return(output)\n",
    "\n",
    "    def _btalgorithm(self,beta_npm ,learning_rate,b,maxiter_bt,rl):\n",
    "\n",
    "        grad_beta = self._grad_L2loss(beta_npm = beta_npm)\n",
    "        for i in range(maxiter_bt):\n",
    "            beta_npm_postgrad = beta_npm - learning_rate * grad_beta\n",
    "            beta_npm_postgrad_postprox = self._prox(beta_npm_postgrad, learning_rate * rl)\n",
    "            fz = self._L2loss(beta_npm_postgrad_postprox)\n",
    "            #fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm_postgrad)\n",
    "            fhatz = self.fhatlambda(learning_rate,beta_npm_postgrad_postprox, beta_npm)\n",
    "            if fz <= fhatz:\n",
    "                #print(i)\n",
    "                break\n",
    "            learning_rate = b*learning_rate\n",
    "\n",
    "        return(beta_npm_postgrad_postprox,learning_rate)\n",
    "\n",
    "    def fit(self, beta0_npm = None):\n",
    "\n",
    "        reg_l1s = self.reg_l1s\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        p = self.p\n",
    "\n",
    "        dg_M = self.dg_M\n",
    "        df_M = self.df_M\n",
    "\n",
    "        tol = self.tol\n",
    "        np.random.RandomState(0)\n",
    "\n",
    "        if beta0_npm is None:\n",
    "            beta_npm_hat = 1 / (n*m*p) * np.random.normal(0.0, 1.0, [n, p,m])\n",
    "            #1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        else:\n",
    "            beta_npm_hat = beta0_npm\n",
    "\n",
    "        fit_params = list()\n",
    "        for l, rl in enumerate(reg_l1s):\n",
    "            fit_params.append({'beta': beta_npm_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_npm_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "\n",
    "            alpha = 1.\n",
    "            beta_npm_hat = fit_params[-1]['beta']\n",
    "            #g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            learning_rate = self.learning_rate\n",
    "            beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "            beta_npm_hat_2 = beta_npm_hat.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                #print(t,l,rl)\n",
    "                #print(t)\n",
    "                L.append(self._loss(beta_npm_hat, rl))\n",
    "                L2.append(self._L2loss(beta_npm_hat))\n",
    "                PEN.append(self._L1penalty(beta_npm_hat))\n",
    "                w = (t / (t+ 3))\n",
    "                beta_npm_hat_momentumguess = beta_npm_hat + w*(beta_npm_hat_1 - beta_npm_hat_2)\n",
    "\n",
    "                beta_npm_hat , learning_rate = self._btalgorithm(beta_npm_hat_momentumguess,learning_rate,.5,1000, rl)\n",
    "                #print(beta_npm_hat_momentumguess.max(), beta_npm_hat.max(),self._L2loss(beta_npm_hat), learning_rate)\n",
    "                beta_npm_hat_2 = beta_npm_hat_1.copy()\n",
    "                beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "\n",
    "            fit_params[-1]['beta'] = beta_npm_hat\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "\n",
    "        self.fit_ = fit_params\n",
    "        #self.ynull_ = np.mean(y)\n",
    "\n",
    "        return self\n",
    "\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89c88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function multivariate_normal:\n",
      "\n",
      "multivariate_normal(...) method of numpy.random.mtrand.RandomState instance\n",
      "    multivariate_normal(mean, cov, size=None, check_valid='warn', tol=1e-8)\n",
      "    \n",
      "    Draw random samples from a multivariate normal distribution.\n",
      "    \n",
      "    The multivariate normal, multinormal or Gaussian distribution is a\n",
      "    generalization of the one-dimensional normal distribution to higher\n",
      "    dimensions.  Such a distribution is specified by its mean and\n",
      "    covariance matrix.  These parameters are analogous to the mean\n",
      "    (average or \"center\") and variance (standard deviation, or \"width,\"\n",
      "    squared) of the one-dimensional normal distribution.\n",
      "    \n",
      "    .. note::\n",
      "        New code should use the ``multivariate_normal`` method of a ``default_rng()``\n",
      "        instance instead; please see the :ref:`random-quick-start`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    mean : 1-D array_like, of length N\n",
      "        Mean of the N-dimensional distribution.\n",
      "    cov : 2-D array_like, of shape (N, N)\n",
      "        Covariance matrix of the distribution. It must be symmetric and\n",
      "        positive-semidefinite for proper sampling.\n",
      "    size : int or tuple of ints, optional\n",
      "        Given a shape of, for example, ``(m,n,k)``, ``m*n*k`` samples are\n",
      "        generated, and packed in an `m`-by-`n`-by-`k` arrangement.  Because\n",
      "        each sample is `N`-dimensional, the output shape is ``(m,n,k,N)``.\n",
      "        If no shape is specified, a single (`N`-D) sample is returned.\n",
      "    check_valid : { 'warn', 'raise', 'ignore' }, optional\n",
      "        Behavior when the covariance matrix is not positive semidefinite.\n",
      "    tol : float, optional\n",
      "        Tolerance when checking the singular values in covariance matrix.\n",
      "        cov is cast to double before the check.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        The drawn samples, of shape *size*, if that was provided.  If not,\n",
      "        the shape is ``(N,)``.\n",
      "    \n",
      "        In other words, each entry ``out[i,j,...,:]`` is an N-dimensional\n",
      "        value drawn from the distribution.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Generator.multivariate_normal: which should be used for new code.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The mean is a coordinate in N-dimensional space, which represents the\n",
      "    location where samples are most likely to be generated.  This is\n",
      "    analogous to the peak of the bell curve for the one-dimensional or\n",
      "    univariate normal distribution.\n",
      "    \n",
      "    Covariance indicates the level to which two variables vary together.\n",
      "    From the multivariate normal distribution, we draw N-dimensional\n",
      "    samples, :math:`X = [x_1, x_2, ... x_N]`.  The covariance matrix\n",
      "    element :math:`C_{ij}` is the covariance of :math:`x_i` and :math:`x_j`.\n",
      "    The element :math:`C_{ii}` is the variance of :math:`x_i` (i.e. its\n",
      "    \"spread\").\n",
      "    \n",
      "    Instead of specifying the full covariance matrix, popular\n",
      "    approximations include:\n",
      "    \n",
      "      - Spherical covariance (`cov` is a multiple of the identity matrix)\n",
      "      - Diagonal covariance (`cov` has non-negative elements, and only on\n",
      "        the diagonal)\n",
      "    \n",
      "    This geometrical property can be seen in two dimensions by plotting\n",
      "    generated data-points:\n",
      "    \n",
      "    >>> mean = [0, 0]\n",
      "    >>> cov = [[1, 0], [0, 100]]  # diagonal covariance\n",
      "    \n",
      "    Diagonal covariance means that points are oriented along x or y-axis:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
      "    >>> plt.plot(x, y, 'x')\n",
      "    >>> plt.axis('equal')\n",
      "    >>> plt.show()\n",
      "    \n",
      "    Note that the covariance matrix must be positive semidefinite (a.k.a.\n",
      "    nonnegative-definite). Otherwise, the behavior of this method is\n",
      "    undefined and backwards compatibility is not guaranteed.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Papoulis, A., \"Probability, Random Variables, and Stochastic\n",
      "           Processes,\" 3rd ed., New York: McGraw-Hill, 1991.\n",
      "    .. [2] Duda, R. O., Hart, P. E., and Stork, D. G., \"Pattern\n",
      "           Classification,\" 2nd ed., New York: Wiley, 2001.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> mean = (1, 2)\n",
      "    >>> cov = [[1, 0], [0, 1]]\n",
      "    >>> x = np.random.multivariate_normal(mean, cov, (3, 3))\n",
      "    >>> x.shape\n",
      "    (3, 3, 2)\n",
      "    \n",
      "    The following is probably true, given that 0.6 is roughly twice the\n",
      "    standard deviation:\n",
      "    \n",
      "    >>> list((x[0,0,:] - mean) < 0.6)\n",
      "    [True, True] # random\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "help(np.random.multivariate_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f975dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "d = 2\n",
    "p = 3\n",
    "\n",
    "# # sample_grads = .1* np.random.multivariate_normal(np.zeros(d), np.identity(d),p)\n",
    "\n",
    "\n",
    "# dg_M = np.zeros((n,d ,p))\n",
    "# dg_M[0] = rearrange(sample_grads, 'p d -> d p')\n",
    "df_M = np.zeros((n,d ,d))\n",
    "df_M[0] = np.identity(d)\n",
    "# df_M[0] = [[np.sqrt(2) / 2, np.sqrt(2) / 2],[np.sqrt(2) / 2, -np.sqrt(2) / 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6124a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3697300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_M = np.zeros((n,d,p))\n",
    "# dg_M[0,:,0] = [.5,0]\n",
    "# dg_M[0,:,1] = [0,.5]\n",
    "# delta = .5\n",
    "# mag = .5 + delta\n",
    "dg_M[0,:,0] = [1.,0]\n",
    "dg_M[0,:,1] = [0,1.]\n",
    "delta = .5\n",
    "mag = 1. + delta\n",
    "angle = np.pi / 4\n",
    "# angle = np.pi / 12\n",
    "dg_M[0,:,2] = [np.sin(angle) * mag,np.cos(angle) * mag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb8c5bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientGroupLasso at 0x7fbc614e7198>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl_itermax = 100000\n",
    "tol = 1e-16\n",
    "learning_rate = 10000\n",
    "GGL = GradientGroupLasso(dg_M, df_M, np.asarray([.00001]), 0., gl_itermax,learning_rate, tol, beta0_npm= None)\n",
    "GGL.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8de72901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.77445424, -0.22639536],\n",
       "        [-0.22553337,  0.77359224],\n",
       "        [ 0.21263758,  0.21345027]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL.fit_[0]['beta'] # we can't just put all the weight on the big one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b74f3237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4095500082301197e-05"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(df_M[0] - np.einsum('i p d, i e p -> i d e',GGL.fit_[0]['beta'], dg_M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08d9dc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4260630631947175"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(GGL.fit_[0]['beta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15964052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.913954977788813"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL._L1penalty(GGL.fit_[0]['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f288785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1\n",
    "# d = 4\n",
    "# p = 30\n",
    "# sample_grads = .1* np.random.multivariate_normal(np.zeros(d), np.identity(d),p)\n",
    "\n",
    "# dg_M = np.zeros((n,d ,p))\n",
    "# dg_M[0] = rearrange(sample_grads, 'p d -> d p')\n",
    "# df_M = np.zeros((n,d ,d))\n",
    "# df_M[0] = np.identity(d)\n",
    "\n",
    "# df_M = np.zeros((n,d ,d))\n",
    "# df_M[0] = np.identity(2)\n",
    "# df_M[1] = np.identity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c735aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_itermax = 100000\n",
    "tol = 1e-16\n",
    "learning_rate = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6e4dbb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientGroupLasso at 0x7faefcb7e048>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL = GradientGroupLasso(dg_M, df_M, np.asarray([.00001]), 0., gl_itermax,learning_rate, tol, beta0_npm= None)\n",
    "GGL.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "abd50367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0746825 , 0.13848074, 0.15381845, 0.09953057, 0.16461224,\n",
       "        0.13645059, 0.11598697, 0.13391328, 0.09978519, 0.12043461,\n",
       "        0.14850199, 0.26623018, 0.0785493 , 0.27583127, 0.23963432,\n",
       "        0.17556931, 0.11773368, 0.08180694, 0.06680022, 0.18362351,\n",
       "        0.17059862, 0.30011823, 0.19452131, 0.19509776, 0.28501958,\n",
       "        0.19935068, 0.14379883, 0.1708637 , 0.25582961, 0.12222831]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(dg_M , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fd0324c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldnt we just keep the longest most orthogonal pair?  No.\n",
    "# it looks like maybe it works...\n",
    "# before we could see that the dual problem minimizer was actually not the cardinality 2 solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b388bfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.688898035659722"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL._L1penalty(GGL.fit_[0]['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c92eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3a0ebab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 1.57705054,  0.26917904,  2.90343879,  1.410141  ],\n",
       "        [-0.38813069, -1.69551913, -2.23436179, -0.95847098],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.90272435,  0.26605372,  0.03071345, -2.17253303],\n",
       "        [ 2.26462341,  1.33263799, -2.94302153, -1.90316585],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 1.43792485, -1.27000434,  0.59720632, -1.52466348],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.06017372, -0.19487281,  1.34877556,  0.04704864],\n",
       "        [-1.09122202,  0.61749545, -0.06383053, -0.74845808],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL.fit_[0]['beta'] # we can't just put all the weight on the big one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bca810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generally speaking we need to retain at least d dictionary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "df0326af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7faefdc5b4e0>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.linalg.norm(dg_M , axis = 1), np.linalg.norm(GGL.fit_[0]['beta'], axis = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a21a2326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80362935, 0.78955648, 0.32090474]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(GGL.fit_[0]['beta'], axis = 2) # the norm on each of the vectors is invariant to the target basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same amount of norm for each of them?  so it doesn't make sense to say mag**(-1) or things like that...\n",
    "# but why would we ever prefer the non big aligned vector for that reconstruction??????\n",
    "# I can add on the other vectors sublinearly due to the 2 norm structure.\n",
    "# the minimum norm inverse is a type of matrix norm?  is it the inverse of a norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16b8b24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.080880226666667"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mag**(-1) + 0.70710678 + 0.70710678 # this is the naive solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7093cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anyway so cause of the symmetry we can think of there as being one coefficient per dictionary function\n",
    "# how many are retained?\n",
    "# so we've considered the intuition where the long one is the aligned one\n",
    "# how about the intuition where the other two are aligned.  why does adding the long one make the penalty smaller\n",
    "# is that case, we can more efficiently use the big one and subtract some of the small ones without accruing much penalty\n",
    "\n",
    "\n",
    "# maybe we are bounded by d^2 functions retained cause n = d but we have d tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5195753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the worst case scenario arrangement of vectors for matching the bp and lasso losses?\n",
    "# does ambient space regression improve the situation?\n",
    "\n",
    "# the main thing id like to say is that lambda close to 0 creates a ton of sparsity (e.g. d^2)\n",
    "# and that the minimizer within that set is close to outside of that set\n",
    "# if i can establish a worst case scenario, then i can analyze how close the minimizer is within the set to outside of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e024d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/miniconda3/envs/montlake_xxx/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.70710678, -0.70710678]),\n",
       " array([], dtype=float64),\n",
       " 2,\n",
       " array([1., 1.]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(dg_M[0][:,:2], df_M[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75cdc0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 1.06066017],\n",
       "       [0.        , 1.        , 1.06066017]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_M[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8554caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 1.  , 1.05]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(dg_M, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dfc0850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.99990074e-01, 3.67556745e-07],\n",
       "        [3.24734653e-07, 9.99990022e-01]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('i p d, i e p -> i d e',GGL.fit_[0]['beta'], dg_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "280afd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always take the longest magnitude vector?  appears angle does not matter for this\n",
    "# rotate target to align with longest vector... then its smaller.  so how is the other smaller?\n",
    "# its orthogonal to the big one and so has to get pushed sideways by the combination of the other two\n",
    "# the loss to go sideways is the same as to go straight?  i.e. 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfea511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "760ffca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "subsets =  list(combinations(list(range(p)), d))\n",
    "asdf = []\n",
    "for subset in subsets:\n",
    "    asdf.append(GGL._L1penalty(np.linalg.inv(dg_M[:,:,subset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44f22098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55acf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "??GGL._L1penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a58e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_indices(lst):\n",
    "    min_value = min(lst)  # Find the minimum value in the list\n",
    "    return [i for i, x in enumerate(lst) if x == min_value]  # List indices where the element is the minimum value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f6a4269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 1), 4.0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets[argmin_indices(asdf)[0]], min(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82d6f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta0_npm = GGL.fit_[-1]['beta']\n",
    "# coeffs[probe_init_low] = GGL.fit_[-1]['beta']\n",
    "# combined_norms[probe_init_low] = np.sqrt((np.linalg.norm(coeffs[probe_init_low], axis = 0)**2).sum(axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72523e",
   "metadata": {},
   "source": [
    "You can express any $x_j = \\gamma^T x_{[d]}$ where $x_{[d]} \\in R^{d \\times d}$.\n",
    "\n",
    "Can we express certain $x_j$ more cheaply i.e. with $\\|\\gamma\\|_2 < 1$? No, not always (think about |<).\n",
    "\n",
    "But even in that case the ultimate selection is d-sparse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d8570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from montlake.statistics.basispursuit import get_basispursuit_losses\n",
    "resulttts = get_basispursuit_losses(dg_M,df_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d8cd9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.        , 4.45780722],\n",
       "       [0.        , 0.        , 4.45780722],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulttts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1cf50aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_basispursuit_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b167ab65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i should search though all dimensionalities for the basis pursuit losses....\n",
    "# but it is harder since when p > d they are not well defined\n",
    "# i can run lasso on subsets and compared penalties.  the optimum solution should have the lowest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f484abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccec27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Generate synthetic data for p > n situation\n",
    "np.random.seed(42)\n",
    "n, p = 3, 10  # n observations, p predictors\n",
    "\n",
    "X = np.random.randn(n, p)\n",
    "true_beta = np.zeros(p)\n",
    "true_beta[:10] = np.random.randn(10)  # Only first 10 predictors have non-zero coefficients\n",
    "y = X @ true_beta + np.random.randn(n) * 0.1  # Add a bit of noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c32b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Lasso model with a very low lambda (alpha)\n",
    "lasso = Lasso(alpha=4e-6, tol = 1e-16, max_iter = 100000)\n",
    "??lasso.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "71957624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero coefficients: 4\n",
      "   Predictor  Coefficient\n",
      "0          0    -0.540646\n",
      "1          1    -0.000000\n",
      "2          2    -0.000000\n",
      "3          3    -0.774762\n",
      "4          4    -0.000000\n",
      "5          5    -0.000000\n",
      "6          6    -0.016620\n",
      "7          7    -0.000000\n",
      "8          8    -0.000000\n",
      "9          9    -0.024962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/miniconda3/envs/montlake_xxx/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2398393206546015e-06, tolerance: 5.381200667582692e-16\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get the coefficients and count the number of non-zero coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "non_zero_coef_count = np.sum(lasso_coef != 0)\n",
    "\n",
    "# Prepare the results\n",
    "results = pd.DataFrame({\n",
    "    'Predictor': np.arange(p),\n",
    "    'Coefficient': lasso_coef\n",
    "})\n",
    "\n",
    "print(f'Number of non-zero coefficients: {non_zero_coef_count}')\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "7d2dcdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.368458611478848e-06"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(lasso.predict(X) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a97aed49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3569899433347112"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "004a1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr = rearrange(X,'n p -> 1 n p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f6315435",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_arr = rearrange(y, 'n -> 1 n 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "a0712ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged 1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GradientGroupLasso at 0x7faf04282710>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL = GradientGroupLasso(X_arr, y_arr, np.asarray([1e-5]), 0., gl_itermax,learning_rate, tol, beta0_npm= None)\n",
    "GGL.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "149ef324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7faf009f0320>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(lasso_coef,GGL.fit_[0]['beta'][0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c0552519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.123108491832222e-05"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_arr[0][:,0] - np.einsum('i p j, i n p -> n',GGL.fit_[0]['beta'], X_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "acb1e2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6313208823088736"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(GGL.fit_[0]['beta'])) # penalty is much more minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e3e4274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "class CoordinateDescentGroupLasso:\n",
    "    def __init__(self, dg_M, df_M, reg_l1s, reg_l2, max_iter, tol, beta0_npm=None):\n",
    "        self.dg_M = dg_M\n",
    "        self.df_M = df_M\n",
    "        self.reg_l1s = reg_l1s\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.beta0_npm = beta0_npm\n",
    "        self.n, self.d, self.p = dg_M.shape\n",
    "        self.m = df_M.shape[2]\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.lossresults = {}\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "\n",
    "    def _soft_threshold(self, rho, lam):\n",
    "        return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0)\n",
    "\n",
    "    def _compute_residual(self, beta_npm, j, k):\n",
    "        print(beta_npm.shape)\n",
    "        df_M_hat = np.einsum('n d p,n p m->n d m', self.dg_M, beta_npm)\n",
    "        print(self.dg_M.shape)\n",
    "        return self.df_M[:, :, k] - df_M_hat[:, :, k] + np.einsum('n d p,n -> n d', self.dg_M, beta_npm[:, j, k])\n",
    "\n",
    "    def _coordinate_update(self, beta_npm, j, k, lam):\n",
    "        residual = self._compute_residual(beta_npm, j, k)\n",
    "        rho = np.einsum('n d,n d->', self.dg_M[:, :, j], residual)\n",
    "        return self._soft_threshold(rho, lam) / (np.linalg.norm(self.dg_M[:, :, j]) ** 2 + self.reg_l2)\n",
    "\n",
    "    def _loss(self, beta_npm, reg_lambda):\n",
    "        df_M_hat = np.einsum('n d p,n p m -> n d m', self.dg_M, beta_npm)\n",
    "        residual = self.df_M - df_M_hat\n",
    "        l2_loss = 0.5 * np.linalg.norm(residual) ** 2\n",
    "        l1_penalty = reg_lambda * np.linalg.norm(rearrange(beta_npm, 'n p m -> (m n) p'), axis=0).sum()\n",
    "        return l2_loss + l1_penalty\n",
    "\n",
    "    def fit(self, beta0_npm=None):\n",
    "        if beta0_npm is None:\n",
    "            beta_npm_hat = np.zeros((self.n, self.p, self.m))\n",
    "        else:\n",
    "            beta_npm_hat = beta0_npm\n",
    "\n",
    "        for rl in self.reg_l1s:\n",
    "            L = []\n",
    "\n",
    "            for t in range(self.max_iter):\n",
    "                beta_npm_old = beta_npm_hat.copy()\n",
    "\n",
    "                for j in range(self.p):\n",
    "                    for k in range(self.m):\n",
    "                        beta_npm_hat[:, j, k] = self._coordinate_update(beta_npm_hat, j, k, rl)\n",
    "\n",
    "                L.append(self._loss(beta_npm_hat, rl))\n",
    "\n",
    "                if t > 0 and np.abs(L[-1] - L[-2]) / L[-1] < self.tol:\n",
    "                    break\n",
    "\n",
    "            self.lossresults[rl] = L\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bd5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3fb24dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 10)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CGGL.dg_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b660d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (montlake)",
   "language": "python",
   "name": "montlake_xxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
