{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff853350",
   "metadata": {},
   "source": [
    "So far I've looked at a method for proving the D-unitary optimality theorem from the perspective of contradiction.  Suppose I have a solution to the optimization problem, and I make a version of it that is just a copy with a unitary matrix plugged in somewhere in the estimated support.  I wanted to say that the loss from the adjusted support is less than the loss from the original.  But this is not necessarily the case since the loss may be increase by substituting in a new unitary matrix for another, when y is not unitary.  I don't have a solution to this problem, but the general principles that make me think this theorem is true (epsilon small violations in computation, elegance, uniqueness of GL solution) make me think its still worth proving.\n",
    "\n",
    "As an alternative approach, I'm considering evaluating the partial derivatives of the Lagrangian formulation of the problem and trying to show that D element solution is a local optimum in the sense that the gradient is zero with respect to all the parameters, including the data, which is to be evaluated in a space quotiented by unitary matrices.  Of course, this is only a global optimum if the problem is convex in both X and beta.  I'd like to understand this joint convexity today.\n",
    "\n",
    "Pseudoinversion of a PSD matrix seems to be convex.  However, there is no requirement for X to be PSD, and the correspondence between convexity of the pseudoinverse constraint and convexity of the pseudoinverse map isn't obvious to me right now.  If the overall problem is non-convex, it is possible the iterative algorithm I tried to use in my contradiction proof may have a dynamic programming interpretation as a way to get around this.\n",
    "\n",
    "Let's try a very simple example, showing that the KKT condition (ie beta only) is satisfied by beta identity when X is identity.  The problem we have is\n",
    "\n",
    "\\begin{align}\n",
    "\\min f(\\beta) \\; : \\; x \\beta = I_D\n",
    "\\end{align}\n",
    "\n",
    "where $f(\\beta) = \\| \\beta \\|_{1,2}$.\n",
    "Let's write out the Lagrangian.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta, x, \\Lambda) &= f(\\beta) + \\sum_{d, d'=1}^D \\Lambda_{dd'} (x \\beta - I_D)_{dd'} \\\\\n",
    "&=  f(\\beta) + \\text{tr}( \\Lambda^T (x \\beta - I_D)),\n",
    "\\end{align}\n",
    "\n",
    "Recall that the Lagrangian has form such that \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\beta} &= \\frac{\\partial f}{\\partial \\beta} + x^T \\Lambda \\\\\n",
    "\\frac{\\partial L}{\\partial x} &= \\Lambda \\beta^T \\\\\n",
    "\\frac{\\partial L}{\\partial \\Lambda} &= x\\beta - I_D \\\\\n",
    "\\end{align}\n",
    "\n",
    "so $\\frac{\\partial L}{\\partial \\beta, x , \\Lambda} = 0$ implies that\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\beta} &=  - x^T \\Lambda  \\text{ stationarity pt 1} \\\\\n",
    "\\Lambda \\beta^T  &= 0  \\text{ stationarity pt 2} \\\\\n",
    "x\\beta &= I_D  \\text{ primal feasibility}  \n",
    "\\end{align}\n",
    "\n",
    "This corresponds to the fact that extremal points will be located when the level sets of the objective are tangent to the constraint.\n",
    "To see why, recall that if the level sets were not tangent, we could move to a preferred level set while remaining on the constraint.\n",
    "\n",
    "To complete the characterization, note that \n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\beta} &= \\frac{\\partial \\| \\beta \\|_{1,2}}{\\partial \\beta} \\\\\n",
    "&= \\left\\{ g \\in \\mathbb{R}^{P \\times D} \\mid g_{ij} = \\frac{\\beta_{ij}}{\\|\\beta_{i\\cdot}\\|_2} \\ \\text{if} \\ \\|\\beta_{i\\cdot}\\|_2 \\neq 0; \\ \\text{any} \\ g_{ij} \\ \\text{with} \\ \\|g_{i\\cdot}\\|_2 \\leq 1 \\ \\text{if} \\ \\|\\beta_{i\\cdot}\\|_2 = 0 \\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "Now lets take the example of $x = \\beta = I_1$.  Then $\\Lambda = [[0]]$ from the second stationarity condition.  But then $\\frac{\\partial f}{\\partial \\beta} = I_1$ so $\\Lambda = [[0]]$ and so how is stationarity pt 1 satisfied?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a4a71",
   "metadata": {},
   "source": [
    "Show the KKT conditions for a basis pursuit solution to\n",
    "\\begin{align}\n",
    "\\min \\|\\beta\\|_1 \\; : \\; x \\beta = 1\n",
    "\\end{align}\n",
    "are satisfied by $\\beta = 1$ when $x = 1$.\n",
    "\n",
    "I think that may be a problem since I can't get a tangent space of the constraint, as it is exactly solvable.\n",
    "Let's try $\\beta = [1, 0], x = [1, 1/2]$.\n",
    "\n",
    "Show the KKT conditions for a basis pursuit solution to\n",
    "\\begin{align}\n",
    "\\min \\|\\beta\\|_1 \\; : \\; x \\beta = 1\n",
    "\\end{align}\n",
    "are satisfied by $\\beta = [1, 0], x = [1, 1/2]$.\n",
    "\n",
    "Okay, let's try \n",
    "\\begin{align}\n",
    "\\frac{\\partial f }{\\partial \\beta} = [1, g] \n",
    "\\end{align}\n",
    "with $g \\in [-1,1]$.\n",
    "\n",
    "$[1, g] = -[1, 1/2]^T \\Lambda$ implies $ \\Lambda = -1$ and so $g = -1/2$.\n",
    "\n",
    "Okay, let's try \n",
    "\\begin{align}\n",
    "\\frac{\\partial f }{\\partial \\beta} = [g,2] \n",
    "\\end{align}\n",
    "with $g \\in [-1,1]$.\n",
    "\n",
    "$[g,2 ] = -[1, 1/2]^T \\Lambda$ implies $ \\Lambda = -4$ and so $g = 4$ which isn't possible.\n",
    "So we seem to be making progress.\n",
    "\n",
    "If we differentiate w.r.t. $x$ as well I don't think this will work.  But we can add quadratic constraints to keep vector norms below $1$?\n",
    "\n",
    "\"The Lagrange Multipliers are the magnitude of the gradient of the objective function in the normal space to the constraint function\"\n",
    "\n",
    "The flexibility in subgradient satisfaction gives the sparsity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a086154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea5211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b281e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f43bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78484ad4",
   "metadata": {},
   "source": [
    "Let's write out the constrained Lagrangian.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta, x, \\Lambda) &= f(\\beta) + \\sum_{d, d'=1}^D \\Lambda_{dd'} (x \\beta - I_D)_{dd'} + \\sum_{p=1}^P \\mu_j (\\|x_j\\| - 1)\n",
    "\\end{align}\n",
    "\n",
    "First, let's investigate whether our putative solution satisfies the so called dual feasiblity $(\\mu_j \\geq 0 )$ and complementary slackness $(\\mu_j (\\|x_j\\| - 1) = 0 )$ conditions.  These seem both satisfiable, although $\\mu_1$ could be non-zero.\n",
    "\n",
    "\n",
    "Recall that $ \\Lambda = -1$ and so $g = -1/2$.\n",
    "\n",
    "We seek to show that $\\beta = [1, 0], x = [1, 1/2]$ is a solution even when minizing over $x$ as well.\n",
    "\n",
    "Namely, now \n",
    "$\\frac{\\partial L}{\\partial x} = \\Lambda \\beta^T + 2 \\mu x$ or equivalently $\\Lambda \\beta^T = - 2 \\mu x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d5086",
   "metadata": {},
   "source": [
    "What about when $x = [I_D, 2I_D, 0], \\beta = [I_D, 0, 0], \\Lambda = [[I_D, 0 ,0],[0,0, 0],[0,0,0]]$. But this shouldn't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd83a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isometry",
   "language": "python",
   "name": "isometry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
