\section{Supplement}

This section contains algorithms, proofs, and experiments in support of the main text.

\subsection{Algorithms}
\label{sec:algorithms}

We give definitions of the brute and greedy algorithms for the combinatorial problem studied in this paper.
The brute force algorithm is computationally intractable for all but the smallest problems, but always finds the global minima.

\begin{algorithm}[H]
\caption{\brute(Matrix ${X} \in \mathbb{R}^{D \times P}$, objective $f$)}
\begin{algorithmic}[1]
\FOR{each combination $S \subseteq \{1, 2, \dots, P\}$ with $|S| = D$}
    \STATE Evaluate $f({X}_{.S})$
\ENDFOR
\STATE {\bf Output} the combination $S^*$ that minimizes $f({X}_{.S})$
\end{algorithmic}
\end{algorithm}

Greedy algorithms are computationally expedient but can get stuck in local optima \citep{Cormen, Russell-09}, even with randomized restarts \citep{Dick2014HowMR}.

\begin{algorithm}[H]
\caption{\greedy(Matrix ${X} \in \mathbb{R}^{D \times P}$, objective $f$, selected set $S = \emptyset$, current size $d=0$)}
\begin{algorithmic}[1]
\IF{$d = D$}
    \STATE {\bf Return} $S$
\ELSE
    \STATE {\bf Initialize} $S_{\text{best}} = S$
    \STATE {\bf Initialize} $f_{\text{best}} = \infty$
    \FOR{each $p \in \{1, 2, \dots, P\} \setminus S$}
        \STATE {\bf Evaluate} $f({X}_{.(S \cup \{p\})})$
        \IF{$f({X}_{.(S \cup \{p\})}) < f_{\text{best}}$}
            \STATE {\bf Update} $S_{\text{best}} = S \cup \{p\}$
            \STATE {\bf Update} $f_{\text{best}} = f(\mathcal{X}_{.(S \cup \{p\})})$
        \ENDIF
    \ENDFOR
    \STATE {\bf Return} \greedy(${X}$, $f$, $S_{\text{best}}$, $d+1$)
\ENDIF
\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Proofs}
\label{sec:proofs}

\subsubsection{Proof of Proposition \ref{prop:basis_pursuit_selection_invariance}}
\label{proof:basis_pursuit_program_invariance}

In this proof we first show that the penalty $\|\beta\|_{1,2}$ is unchanged by unitary transformation of $\beta$.

 \begin{proposition}
 \label{prop:basis_pursuit_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\|\beta\|_{1,2} = \|\beta U \|$.
\end{proposition}

\begin{proof}
\begin{align}
\|\beta U \|_{1,2} &= \sum_{p = 1}^P \| \beta_{p.} U \| \\
&= \sum_{p = 1}^P \| \beta_{p.} \| \\
&= \|\beta \|_{1,2}
\end{align}
\end{proof}

We then show that this implies that the resultant loss is unchanged by unitary transformation of $ X$.

\begin{proposition}
 \label{prop:basis_pursuit_loss_equivalence}
 Let $U \in \mathbb R^{D \times D}$ be unitary.
 Then $\widehat \beta  (U  X) = \widehat \beta  (  X) U$.
\end{proposition}

\begin{proof}
\begin{align}
\widehat \beta  (U  X)  &= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \; I_{D} = U X \beta \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \; U^{-1} U = U^{-1} U X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta\|_{1,2}  \; : \;  I_D = X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta U \|_{1,2}  \; : \;  I_D = X \beta U \\
&= \arg \min_{\beta \in \mathbb R^{P \times D}} \|\beta \|_{1,2}  \; : \;  I_D = X \beta.
\end{align}
\end{proof}

\subsubsection{Proof of Proposition \ref{prop:unitary_selection}}
\label{sec:local_isometry_proof}

 \begin{proposition}
\label{prop:generalized_unitary_selection}
Let $w_c$ be a normalization satisfying the conditions in Definition \ref{def:symmetric_normalization}.
Let $X \in \mathbb R^{D \times P}$ contain a rank $D$ orthonormal submatrix $X_{.S} \in \mathbb R^{D\times D}$.
Then $ S \subseteq S( \{\arg \min_{\beta \in \mathbb R^{D \times P}} \|\beta\|_{1,2} :  I_D = \; w(X,c) \beta\} \; D)$.
 \end{proposition}
 
 \begin{proof}
 
 Without loss of generality, we show that $[I_D 0]$ satisfies the KKT conditions for $X = [I_D X_{-S}]$
 The KKT conditions are 
 
\begin{itemize}
\item Primal feasibility: $w(X, c) \beta = I_D$
\item Stationarity: there exists a dual variable $\nu \in \mathbb{R}^{D \times D}$ such that $0 \in \partial \|\beta\|_{1,2} - w(X, c)^T \nu$  where $\partial$ is the subdifferential operator.
\item Dual feasibility
\item Complementary slackness
\end{itemize}

Stationarity: 
\begin{align}
\|\beta\|_{1,2} = \sum_{p = 1}^P \|\beta_{p.}\|_2.
\end{align}
Then
\begin{align}
\partial \|\beta\|_{1,2} = \begin{bmatrix}
I_D
v_{d+1} \\
\dotsc \\
v_P
\end{bmatrix}
where $v_p \in \mathbb R^{D}$ satisfies $\|v_p\|_2 \leq 1$.
\end{align}
We therefore must satisfy
\begin{align}
\begin{bmatrix}
I_D \\
w(X_{-S},c)^T
\end{bmatrix}\nu  = \begin{bmatrix}
I_D \\
V_{-S}
\end{bmatrix}
\end{align}
where $\nu \in \mathbb R^{D\times D}$.

At this point we can see that in fact $I_D$ is an appropriate choice of $\nu$ by the boundary on the range of the normalized vector.
 %%%%
From the conditions in Definition \ref{def:symmetric_normalization}, normalized matrices will consist of vectors of maximum length (i.e. $1$) if and only if the original matrix also consists of vectors of length $1$.
Such vectors will result in lower basis pursuit loss, since longer vectors in $X$ require smaller corresponding covectors in $\beta$ to equal the same result.
Therefore, it remains to show that $\beta$ consisting of $D$ orthogonal vectors of length $1$ have lower loss compared with $\beta$ with $|S(\beta)| \geq D$ consisting of potentially non-orthogonal vectors of length $1$.

We begin with a proposition on the loss obtained by unitary solution
\begin{proposition}
\label{prop:minimum_loss}
Given a rank $D$ (i.e. invertible) matrix $X_S \in \mathbb R^{D \times D}, (\|\beta\|_{1,2} : \beta = X_S^{-1)) = D$
The value of $D$ is  obtained by $\beta$ orthonormal.
\end{proposition}
\begin{proof}
By Proposition \ref{prop:basis_pursuit_selection_invariance}, for $X$ orthogonal, without loss of generality 
\begin{align}
\beta_{dd'} = \begin{cases} 1 & d = d' \in \{ 1 \dots D\}  \\
0 & \text{otherwise}
\end{cases}.
\end{align}
\end{proof}

We then show that this is a lower bound on the obtained loss.
Since this solution is obtained by a unitary matrix, then the unitary solution will be in the solution set.

To argue for this property, we claim that we can start with a non-D sparse solution, and through a sequence of penalty-preserving operations, eventually end up with a $D$ sparse (i.e.) isometry solution.
Recall that we want to show that the D sparse solution is contained within the set of solutions.
Assume we are given a solution $S'$. % prob should be S and the other one S^* or something
$S$ could intersect $S'$, be contained by it, or be disjoint from it.
Regardless, the operations we consider will be on $X_{S \cup S'}$.

Consider the transformed matrix $X_S^{-1} X$
By Proposition \ref{prop:basis_pursuit_loss_equivalence}, the loss accrued by the solution to $X_S^{-1} X$ will be equivalent to the loss accrued by the solution to $X$ (i.e. $X_{S'}$).
Not only that, the support set of a solution to $X$ will also provide a solution to $X_{S}^{-1} X$, with coefficients given by $\beta_{S'} X_S$.
That is, if $\beta$ is a a solution to $X$, then $\beta X_S$ is a solution to $X_S^{-1} X$.

The nice thing about the transformed problem is we can load up the covectors in $S$ with the remaining coefficient.
Let $y = I_D - X_S \beta_S X_S$.
This is the residual given by the elements of the support that are not in $S$.
Now $X_{S'} \beta_{S'} X_S = y$.

%This feels like it needs to use the configuration of $X_S$ (i.e. the special piece of knowledge about orthants).

%although we can maybe playthe same game with X_S >_.._<
%Can I assume X_S = I_D?
%While later we will show that the D sparse solution is contained within the solution in practice.

The uniqueness of the solution bears attention.
A well-known result in Lasso literature states that if the columns of $X$ are in so-called general position (meaning that no more than $k+1$ columns are contained within any $k$ dimensional subspace, with $k < D$), then the standard lasso solution is unique.
This result has been generalized to show that the lasso solution is unique if and only if the codimension of the intersection of the row span of $X$ with the $P$ dimensional unit cube does not exceed the dimension of the row span of $X$.
That is, the rowspan of $X$ must not intersect too many corners of the $P$ dimensional cube.
In our case, at least the corner among indices $S$ is obtainable, so this codimension is at least one.
Since we can rewrite our multitask problem as a block sparse problem with grouping across blocks, we can see that after a point, additional dimensions mandate that the solution be non-unique.

Notwithstanding the possible presence of non-unique solutions, we would like to show that the isometry solution is on of the solutions.
Clearly, the isometry solution has loss $D$.
We therefore need only to show that all solutions have loss greater than or equal to $D$.
Our general procedure will be to take a solution and operate on it to find a sparser solution of equal loss, and show that eventually a $D$ sparse solution can be arrived at, where the output is unique.

In order to demonstrate the generalizabiliity of this procedure and the sort of moves which are permitted, we need to represent non-unique solutions in a sufficiently general way.
Non-unique solutions arise when every orthant of a $k$ dimension half space contain a vector.


%we have at least two ways.  besides the above (classify non-unique solutions, consider moves that leverage the specific structure of the non-unique solution, I was also just thinking I could kind of start with solution, make moves, yadida
 %I was going to decrease norm everytime I took a \beta_p \otimes X_{.p} thing and regressed it on X_S = I_D.
%There is a special property of our non-unique solutions
%$D = \|I_D\|_{1,2} = \|\beta_{S.} + \beta_{-S} \otimes X_{.-S}\|_{1,2} \leq \|\beta\|_{1,2}$
%Proceeding feature by feature iteratively actually increased norms sometimes if I recall.
%But the above is clearly what we want to show and plausibly true.
 
% We have $\|I_D\|_{1,2} = \|w(X, c)\beta\|_{1,2}$.
% Then I'd love to say $\|I_D\|_{1,2} \leq \|w(X_c)\|_{\infty,2} \|\beta\|_{1,2}$


%This approach (suggested by ChatGPT) is logically unsound
%Each basis vector $e_d$ in $I_D$ is constructed as a sum $e_d = \sum_{p = 1}^P \beta_{pd} X_{.d}$.
%Thus, 
%\begin{align}
%\|e_d\| &= \|\sum_{p=1}^P \beta_{pd} X_{.d}\|
%\end{align}
%We can use the triangular inequality to get the bound
%\begin{align}
%\|\sum_{p=1}^P \beta_{pd} X_{.d}\| &\leq \sum_{p=1}^P \|\beta_{pd} X_{.d}\| \\
%&= \sum_{p=1}^P | \beta_{pd}| \|X_{.d}\| \\
%&= \sum_{p=1}^P | \beta_{pd}| 
%\end{align}
%Then, since $\|e_d\|  = 1$ we have that $\sum_{p=1}^P | \beta_{pd}| \geq 1$.
%This doesn't imply that $\sum_{p=1}^P \|\beta_{p.}\| \geq D$.
%Take for example when you have a $2\times 2$ matrix with entires $.5$.
%but this couldn't be the inverse

%are the values in the inverse themselves bounded?

%The QR decomposition approach has a nice intuition.
%But there was a part of it that broke down
%I forget where however...



Invertible matrices $X_{.S}$ admit QR decompositions $\tilde X_{.S} = QR$ where $Q$ and $R$ are orthonormal and upper-triangular matrices, respectively \citep{Anderson1992-fb}.
Denoting $Q$ to be composed of basis vectors $[e_1 \dots e_D]$, the matrix $R$ has form
\begin{align}
R = \begin{bmatrix}
\langle e_1, X_{.S_1} \rangle & \langle e_1,  X_{.S_2} \rangle  &\dots &  \langle e_1,  X_{.S_D} \rangle \\
0 & \langle e_d,  X_{.S_2} \rangle & \dots  &  \langle e_2,  X_{.S_D} \rangle\\
0 & 0 & \dots & \dots  \\
\dots & \dots & \dots & \dots \\
0 & 0 & \dots & \langle e_D, X_{.S_D} \rangle 
\end{bmatrix}.
\end{align}
Thus, $|R_{dd} | \leq \|X_{.{S_{d}}}\|_2$ for all $d \in [D]$, with equality obtained only by orthonormal matrices.
On the other hand, by Proposition \ref{prop:basis_pursuit_selection_invariance}, $l_c(X) = l_c(R)$ and so $\|\beta\|_{1,2} = \|R^{-1}\|_{1,2}$.
Since $R$ is upper triangular it has diagonal elements $\beta_{dd} = R_{dd}^{-1}$ and so $\|\beta_{d.}\| \geq \| X_{.{S_d}}\|^{-1} = 1$.
That is, the penalty accrued by a particular covector in $\beta$ is bounded from below by $1$ - the inverse of the length of the corresponding vector in $X_{.S}$ - with equality occurring only when $X_{.S}$ is orthonormal.
\end{proof}

\newpage

\subsection{Support cardinalities}
\label{sec:support_cardinalities}

Figure \ref{fig:support_cardinalities} plots the distribution of $|\widehat{S}_{IP}|$ from Table \ref{tab:experiments} in order to contextualize the reported means.
While typically $|\widehat{S}_{IP}| << P$, there are cases for Ethanol where this is not the case that drive up the means.

\begin{figure}[t]
    \centering
    % Subfigure for Wine dataset
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/wine_cardinalities}
        \caption{Wine Dataset}
        \label{fig:wine_cardinalities}
    \end{subfigure}
    \hfill
    % Subfigure for Iris dataset
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{..//figures/iris_cardinalities}
        \caption{Iris Dataset}
        \label{fig:iris_cardinalities}
    \end{subfigure}
    \hfill
    % Subfigure for Ethanol dataset
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/ethanol_cardinalities}
        \caption{Ethanol Dataset}
        \label{fig:ethanol_cardinalities}
    \end{subfigure}
    \caption{Support Cardinalities for Wine, Iris, and Ethanol datasets}
    \label{fig:support_cardinalities}
\end{figure}

\newpage

\subsection{Proposition \ref{prop:unitary_selection} deep dive}
\label{sec:deep_dive}

As mentioned in Section \ref{sec:discussion}, the conditions under which the restriction $P=D$ in Proposition \ref{prop:unitary_selection} may be relaxed are of theoretical and practical interest.
The results in Section \ref{sec:experiments} show that there are circumstances in which the \greedy~ performs better than \tsip, so clearly \tsip~ does not always achieve a global optimum.
Figure \ref{fig:comparison_losses} gives results on the line of inquiry about why this is the case based on the reasoning presented in Section \ref{sec:discussion}.
In these results a two-stage algorithm achieves the global optimum of a slightly different brute problem, namely brute optimization of the multitask basis pursuit penalty $\|\cdot \|_{1,2}$.
That is, brute search on $\|\cdot \|_{1,2}$ gives the same result as the two stage algorithm with brute search on $\|\cdot \|_{1,2}$ subsequent to isometry pursuit.
This suggests that failure to select the global optimum by \tsip~ is in fact only due to the mismatch between global optimums of brute optimization of the multitask penalty and the isometry loss given certain data.
Theoretical formalization, as well as investigation of what data configurations this equivalence holds for, is a logical follow-up.

\begin{figure}[t] % Place at the top of the page
    \centering
    % Top-left plot
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/iris_standardized_0p1_1p0_isometry_losses}
        \caption{Iris Isometry Losses}
        \label{fig:iris_isometry_losses}
    \end{subfigure}
    \hfill
    % Top-right plot
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/iris_standardized_0p1_1p0_group_lasso_losses}
        \caption{Iris Multitask Losses}
        \label{fig:iris_group_lasso_losses}
    \end{subfigure}

    \vspace{0.5cm} % Add vertical spacing between rows

    % Bottom-left plot
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/wine_standardized_0p1_1p0_isometry_losses}
        \caption{Wine Isometry Losses}
        \label{fig:wine_isometry_losses}
    \end{subfigure}
    \hfill
    % Bottom-right plot
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/wine_standardized_0p1_1p0_group_lasso_losses}
        \caption{Wine Multitask Losses}
        \label{fig:wine_group_lasso_losses}
    \end{subfigure}
    \caption{Comparison of Isometry and Group Lasso Losses across $25$ replicates for randomly downsampled Iris and Wine Datasets with $(P,D) = (4,15)$ and $(13, 18)$, respectively.
    Note that this further downsampling compared with Section \ref{sec:experiments} was necessary to compute global minimizers of \brute.
    Lower brute losses are shown with turquoise, while lower two stage losses are shown with pink.
    Equal losses are shown with black lines.}
    \label{fig:comparison_losses}
\end{figure}

\newpage

\subsection{Timing}
\label{sec:timing}

While wall-time of algorithms is a non-theoretical quantity that depends on implementation details, it provides valuable context for practitioners.
We therefore report the following runtimes on a 2021 Macbook Pro.
The particularly high variance for brute force search in the second step of \tsip~ is likely due to the large cardinalities reported in Figure \ref{fig:support_cardinalities}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\toprule
Name & IP & 2nd stage brute & Greedy \\
\midrule
Iris & 1.24 ± 0.02 & 0.00 ± 0.00 & 0.02 ± 0.00 \\
Wine & 2.32 ± 0.17 & 0.13 ± 0.12 & 0.03 ± 0.00 \\
Ethanol & 8.38 ± 0.57 & 0.55 ± 1.08 & 0.07 ± 0.01 \\
\bottomrule
\end{tabular}
\caption{Algorithm runtimes in seconds across replicates.}
\end{table}