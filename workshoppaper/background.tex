\section{Background}

Our algorithm is motivated by spectral and convex analysis.

\subsection{Problem}

Our goal is, given a matrix $ X \in \mathbb R^{D \times P}$, to select a subset $ S \subset [P]$ with $| S| = D$ such that $X_{.  S}$ is as orthonormal as possible in a computationally efficient way.
To that end, we define a ground truth loss function that measures orthonormalness, and then introduce a surrogate loss function that convexifies the problem so that it may be efficiently solved.

\subsection{Interpretability and isometry}

Our motivating example is the selection of data representations from within sets of putative coordinates.
These putative coordinates are simply the columns of a provided wide matrix.
The proposed method is thus even simpler than Sparse PCA \citep{Dey2017-mx, Bertsimas2022-qo, Bertsimas2022-dv}, in which column-covariance is used to select low-dimensional projections from within the span of such a subset.

This method is specifically applicable with respect to interpretability, for which parsimony is at a premium.
Interpretability arises through comparison of data with what is known to be important in the domain of the problem.
This a priori knowledge often takes the form of a functional dictionary.
Regardless of implementation details such as whether this dictionary is given or learned, core concepts like evaluation of independentness of dictionary features arise in numerous scenarios \citep{Chen2019-km, Koelle2022-ju, He2023-ch}.
After functional independence \citep{Koelle2022-ju}, also known as feature decomposability \citep{templeton2024scaling}, which only requires that the differential of sets of dictionary features be full rank, metric properties of such sets are of natural interest.

\begin{definition}
The \textbf{differential} of a smooth map $\phi:\mathcal M \to \mathcal N$ between $D$ dimensional manifolds $\M \subseteq \mathbb R^B$ and $\N \subseteq \mathbb R^P$ is a map in tangent bases $x_1 \dots x_{D}$ of $T_\xi \M$ and $y_1 \dots y_{D}$ of $T_{\phi(\xi)} \N$ consisting of entries
\begin{align}
\label{eq:diff}
    D\phi (\xi) = \begin{bmatrix}
    \frac{\partial \phi_1  }{\partial x_1}(\xi)  & \dots & \frac{\partial \phi_1 }{\partial x_D}(\xi)  \\
    \vdots & & \vdots \\
    \frac{\partial \phi_D }{\partial x_1}(\xi)  & \dots & \frac{\partial \phi_{D}  }{\partial x_{D}}(\xi) 
    \end{bmatrix}.
\end{align}
\end{definition}

\begin{definition}
\label{def:isometric_at_a_point}
A map $\phi$ between $D$ dimensional submanifolds with inherited Euclidean metric $\mathcal M \subseteq R^{B_\alpha}$ and $\mathcal N  \subseteq R^{B_\beta}$ is 
$\phi$ is an \textbf{isometry at a point} $\xi \in \mathcal M$ if
\begin{align}
{D \phi (\xi)}^T D \phi (\xi) = I_D.
\end{align}
That is, $\phi$ is an isometry at $\xi$ if $D \phi (\xi)$ is orthonormal.
\end{definition}

This property - that $D\phi$ is orthonormal, has several equivalent formulations.
The formulation that motivates our ground truth loss function comes from spectral analysis.
\begin{proposition}
\label{prop:orthonormal_spectrum}
The singular values $\sigma_1 \dots \sigma_D$ are equal to $1$ if and only if $U \in \mathbb{R}^{D \times D}$ is orthonormal.
\end{proposition}
On the other hand, the formulation that motivates our convex approach is that orthonormal matrices consist of $D$ coordinate features whose gradients are orthogonal and evenly varying.
\begin{proposition}
\label{prop:orthonormal_basis}
The component vectors $u_1 \dots u_D \in \mathbb R^B$ form a orthonormal matrix if and only if, for all $d_1, d_2 \in [D], \langle u_{d_1}, u_{d_2} \rangle = \begin{cases}
1 \; d_1 = d_2 \\ 
0 \; d_1 \neq d_2 
\end{cases}$.
\end{proposition}

The applications of pointwise isometry are themselves manifold.
Local Tangent Space Alignment \citep{ZhangZ:04}, Multidimensional Scaling \citep{ChenBuja:localMDS09} and Isomap \citep{tenenbaum2000ggf} non-parametrically estimate embeddings that are as isometric as possible.
% TODO (Sam): cite
Pointwise isometries selected from a dictionary may be stitched together to form global embeddings \citep{Kohli2021-lr}.
This approach is particularly relevant since it constructs such isometries through greedy search, with putative dictionary features added one at a time.

Note that it is not necessary to explicitly estimate tangent spaces when applying the definition of isometry.
The most commonly encountered manifolds are simply vector spaces, in which case the tangent spaces are trivial.
This is the case for full-rank tabular data, as well as latent spaces of deep learning models.
For example, the transformer residual stream at different tokens are analogous to tangent spaces of a non-linear manifold in the sense that the relative directions of dictionary vectors are not consistent between tokens.

\subsection{Subset selection}

Given a matrix $ X \in \mathbb R^{D \times P}$, we compare algorithmic paradigms for solving problems of the form
\begin{align}
\label{prog:ground_truth}
\arg \min_{ S \in \binom{[P]}{d}} l ( X_{. S})
\end{align}
where $\binom{[P]}{d} = \left\{ A \subseteq [P] : \left|A\right| = d \right\}$.
Brute force algorithms consider all possible solutions.
These algorithms are conceptually simple, but have the often prohibitive time complexity $O(C_lP^D)$ where $C_l$ is the cost of evaluating $l$.
Greedy algorithms consist of iteratively adding one element at a time to $ S$.
This algorithms have time complexity $O(C_lPD)$ and so are computationally more efficient than brute force algorithms, but can get stuck in local minima.
Please see Section \ref{sec:algorithms} for additional information.

Sometimes, it is possible to introduce an objective which convexifies problems of the above form.
Solutions
\begin{align}
\arg \min f(\beta) : Y  = X\beta 
\end{align}
to the overcomplete regression problem $Y = X \beta$ are a classic example \cite{Chen2001-hh}.
When $f(\beta) = \|\beta\|_0$, this problem is non-convex, and must be solved via greedy or brute algorithms, but when $f(\beta) =\|\beta\|_1$, the problem is convex, and may be solved efficiently via interior-point methods.
When the equality constraint is relaxed, Lagrangian duality may be used to reformulate as a so-called Lasso problem, which leads to an even richer set of optimization algorithms. % cite FISTA< glmnet, coordinate descent
% Cite glmnet


The form of basis pursuit that we apply is inspired by the group basis pursuit approach in \citet{Koelle2022-ju}.
In group basis pursuit (which we call multitask basis pursuit when grouping is dependent only on the structure of matrix-valued response variable $y$) the objective function is $f(\beta) = \|\beta\|_{1,2} := \sum_{p=1}^P \|\beta_{p.}\|_2$  \citep{Yuan2006-bt, Obozinski2006-kq, Yeung2011-fg}
This objective creates joint sparsity across entire rows of $\beta_{p.}$ and was used in \citep{Koelle2022-ju} to select between sets of interpretable features.
% NOTE (Sam): find a citation for multitask basis pursuit.  Cite first GBP paper.