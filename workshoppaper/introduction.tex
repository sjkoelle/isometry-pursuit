\section{Introduction}
\label{sec:introduction}

Many real-world problems may be abstracted as selecting a subset of the columns of a matrix representing stochastic observations or analytically exact data.
This paper focuses on a simple such problem that appears in interpretable learning.
Given a rank $D$ matrix $ X \in \mathbb R^{D \times P}$ with $P > D$, select a square submatrix $ X_{.\mathcal S}$ where subset $\mathcal S \subset P$ satisfies $|\mathcal S| = D$ that is as orthonormal as possible.

This problem arises in interpretable learning because while the coordinate functions of a learned latent space may have no intrinsic meaning, it is sometimes possible to generate a dictionary of interpretable features which may be considered as potential parametrizing coordinates.
When this is the case, selection of candidate interpretable features as coordinates can take the above form.
While implementations vary across data and algorithmic domains, identification of such coordinates generally aids mechanistic understanding, generative control, and statistical efficiency.

This paper shows that an adapted version of the algorithm in \citet{Koelle2024-no} leads to a convex procedure that helps improve upon greedy approaches such as those in \citet{5895106, NEURIPS2019_6a10bbd4, Kohli2021-lr, Jones2007-uc} for finding isometries.
The insight that leads to isometry pursuit is that $D$ function solutions multitask basis pursuit applied to an appropriately normalized $ X$ selects orthonormal submatrices.
In particular, the normalization log-symmetrizes length in the column-space of $ X$ and favors vectors closer to unit length, while basis pursuit favors vectors which are orthogonal.
Our theoretical results formalize this intuition within a limited setting, while our experimental results show the usefulness of isometry pursuit as a trimming procedure prior to brute force search.
Additionally, we introduce a novel ground truth objective function that we measure the success of our algorithm against.
