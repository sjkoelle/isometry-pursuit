
@ARTICLE{Kohli2021-lr,
  title    = "{LDLE}: Low Distortion Local Eigenmaps",
  author   = "Kohli, Dhruv and Cloninger, Alexander and Mishne, Gal",
  abstract = "We present Low Distortion Local Eigenmaps (LDLE), a manifold
              learning technique which constructs a set of low distortion local
              views of a data set in lower dimension and registers them to
              obtain a global embedding. The local views are constructed using
              the global eigenvectors of the graph Laplacian and are registered
              using Procrustes analysis. The choice of these eigenvectors may
              vary across the regions. In contrast to existing techniques, LDLE
              can embed closed and non-orientable manifolds into their
              intrinsic dimension by tearing them apart. It also provides
              gluing instruction on the boundary of the torn embedding to help
              identify the topology of the original manifold. Our experimental
              results will show that LDLE largely preserved distances up to a
              constant scale while other techniques produced higher distortion.
              We also demonstrate that LDLE produces high quality embeddings
              even when the data is noisy or sparse.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  22,
  year     =  2021,
  keywords = "closed manifold; graph laplacian; local parameterization;
              manifold learning; non-orientable manifold; procrustes analysis",
  language = "en"
}


@ARTICLE{Hesterberg2008-iy,
  title         = "Least angle and $\ell_1$ penalized regression: A review",
  author        = "Hesterberg, Tim and Choi, Nam Hee and Meier, Lukas and
                   Fraley, Chris",
  abstract      = "Least Angle Regression is a promising technique for variable
                   selection applications, offering a nice alternative to
                   stepwise regression. It provides an explanation for the
                   similar behavior of LASSO ($\ell_1$-penalized regression)
                   and forward stagewise regression, and provides a fast
                   implementation of both. The idea has caught on rapidly, and
                   sparked a great deal of research interest. In this paper, we
                   give an overview of Least Angle Regression and the current
                   state of related research.",
  month         =  feb,
  year          =  2008,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "0802.0964"
}
@article{HIAI2004155,
title = {Submultiplicativity vs subadditivity for unitarily invariant norms},
journal = {Linear Algebra and its Applications},
volume = {377},
pages = {155-164},
year = {2004},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2003.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0024379503007134},
author = {Fumio Hiai and Xingzhi Zhan},
keywords = {Positive semidefinite matrix, Unitarily invariant norm, Submultiplicativity, Subadditivity, Hadamard product, Matrix Young inequality, Majorization},
abstract = {Let A,B be nonzero positive semidefinite matrices. We prove that∥AB∥∥A∥∥B∥⩽∥A+B∥∥A∥+∥B∥,∥A∘B∥∥A∥∥B∥⩽∥A+B∥∥A∥+∥B∥for any unitarily invariant norm with ∥diag(1,0,…,0)∥⩾1. Some related inequalities are derived.}
}

@ARTICLE{Bertsimas2022-dv,
  title    = "Solving {Large-Scale} Sparse {PCA} to Certifiable (Near)
              Optimality",
  author   = "Bertsimas, Dimitris and Cory-Wright, Ryan and Pauphilet, Jean",
  journal  = "J. Mach. Learn. Res.",
  volume   =  23,
  number   =  13,
  pages    = "1--35",
  year     =  2022
}


@inproceedings{NEURIPS2019_6a10bbd4,
 author = {Chen, Yu-Chia and Meila, Marina},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Selecting the independent coordinates of manifolds with large aspect ratios},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Fazel2001ARM,
  title={A rank minimization heuristic with application to minimum order system approximation},
  author={Maryam Fazel and Haitham A. Hindi and Stephen P. Boyd},
  journal={Proceedings of the 2001 American Control Conference. (Cat. No.01CH37148)},
  year={2001},
  volume={6},
  pages={4734-4739 vol.6},
  url={https://api.semanticscholar.org/CorpusID:6000077}
}

@ARTICLE{Qin2013-tx,
  title     = "Efficient block-coordinate descent algorithms for the Group
               Lasso",
  author    = "Qin, Zhiwei and Scheinberg, Katya and Goldfarb, Donald",
  journal   = "Math. Program. Comput.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  5,
  number    =  2,
  pages     = "143--169",
  month     =  jun,
  year      =  2013,
  language  = "en"
}


@BOOK{Boyd2004-ql,
  title     = "Convex Optimization",
  author    = "Boyd, Stephen P and Vandenberghe, Lieven",
  abstract  = "Convex optimization problems arise frequently in many different
               fields. This book provides a comprehensive introduction to the
               subject, and shows in detail how such problems can be solved
               numerically with great efficiency. The book begins with the
               basic elements of convex sets and functions, and then describes
               various classes of convex optimization problems. Duality and
               approximation techniques are then covered, as are statistical
               estimation techniques. Various geometrical problems are then
               presented, and there is detailed discussion of unconstrained and
               constrained minimization problems, and interior-point methods.
               The focus of the book is on recognizing convex optimization
               problems and then finding the most appropriate technique for
               solving them. It contains many worked examples and homework
               exercises and will appeal to students, researchers and
               practitioners in fields such as engineering, computer science,
               mathematics, statistics, finance and economics.",
  publisher = "Cambridge University Press",
  month     =  mar,
  year      =  2004,
  language  = "en"
}



@ARTICLE{Zhao2023-xn,
  title         = "A Survey of Numerical Algorithms that can Solve the Lasso
                   Problems",
  author        = "Zhao, Yujie and Huo, Xiaoming",
  abstract      = "In statistics, the least absolute shrinkage and selection
                   operator (Lasso) is a regression method that performs both
                   variable selection and regularization. There is a lot of
                   literature available, discussing the statistical properties
                   of the regression coefficients estimated by the Lasso
                   method. However, there lacks a comprehensive review
                   discussing the algorithms to solve the optimization problem
                   in Lasso. In this review, we summarize five representative
                   algorithms to optimize the objective function in Lasso,
                   including the iterative shrinkage threshold algorithm
                   (ISTA), fast iterative shrinkage-thresholding algorithms
                   (FISTA), coordinate gradient descent algorithm (CGDA),
                   smooth L1 algorithm (SLA), and path following algorithm
                   (PFA). Additionally, we also compare their convergence rate,
                   as well as their potential strengths and weakness.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "2303.03576"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Meier2008-ts,
  title    = "The group lasso for logistic regression",
  author   = "Meier, L and van de Geer, Sara and B{\"u}hlmann, P",
  abstract = "Summary. The group lasso is an extension of the lasso to do
              variable selection on (predefined) groups of variables in linear
              regression models. The estimates have the attractive property of
              being invariant under groupwise orthogonal reparameterizations.
              We extend the group lasso to logistic regression models and
              present an efficient algorithm, that is especially suitable for
              high dimensional problems, which can also be applied to
              generalized linear models to solve the corresponding convex
              optimization problem. The group lasso estimator for logistic
              regression is shown to be statistically consistent even if the
              number of predictors is much larger than sample size but with
              sparse true underlying structure. We further use a two‐stage
              procedure which aims for sparser models than the group lasso,
              leading to improved prediction performance for some cases.
              Moreover, owing to the two‐stage nature, the estimates can be
              constructed to be hierarchical. The methods are used on simulated
              and real data sets about splice site detection in DNA sequences.",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume   =  70,
  month    =  feb,
  year     =  2008
}


@ARTICLE{Candes2005-dd,
  title         = "Decoding by Linear Programming",
  author        = "Candes, Emmanuel and Tao, Terence",
  abstract      = "This paper considers the classical error correcting problem
                   which is frequently discussed in coding theory. We wish to
                   recover an input vector $f \in \R^n$ from corrupted
                   measurements $y = A f + e$. Here, $A$ is an $m$ by $n$
                   (coding) matrix and $e$ is an arbitrary and unknown vector
                   of errors. Is it possible to recover $f$ exactly from the
                   data $y$? We prove that under suitable conditions on the
                   coding matrix $A$, the input $f$ is the unique solution to
                   the $\ell_1$-minimization problem ($\|x\|_\{\ell_1\} :=
                   \sum_i |x_i|$) $$ \textbackslashmin_\{g \textbackslashin
                   \textbackslashR^n\} \textbackslash| y - Ag
                   \textbackslash|_\{\textbackslashell_1\} $$ provided that the
                   support of the vector of errors is not too large,
                   $\|e\|_\{\ell_0\} := |\{i : e_i \neq 0\}| \le \rho \cdot m$
                   for some $\rho > 0$. In short, $f$ can be recovered exactly
                   by solving a simple convex optimization problem (which one
                   can recast as a linear program). In addition, numerical
                   experiments suggest that this recovery procedure works
                   unreasonably well; $f$ is recovered exactly even in
                   situations where a significant fraction of the output is
                   corrupted.",
  month         =  feb,
  year          =  2005,
  archivePrefix = "arXiv",
  primaryClass  = "math.MG",
  eprint        = "math/0502327"
}

@ARTICLE{Dey2017-mx,
  title    = "Sparse principal component analysis and its $l_1$-relaxation",
  author   = "Dey, Santanu S and Mazumder, R and Molinaro, M and Wang, Guanyi",
  abstract = "Principal component analysis (PCA) is one of the most widely used
              dimensionality reduction methods in scientific data analysis. In
              many applications, for additional interpretability, it is
              desirable for the factor loadings to be sparse, that is, we solve
              PCA with an additional cardinality (l0) constraint. The resulting
              optimization problem is called the sparse principal component
              analysis (SPCA). One popular approach to achieve sparsity is to
              replace the l0 constraint by an l1 constraint. In this paper, we
              prove that, independent of the data, the optimal objective
              function value of the problem with l0 constraint is within a
              constant factor of the the optimal objective function value of
              the problem with l1 constraint. To the best of our knowledge,
              this is the first formal relationship established between the l0
              and the l1 constraint version of the problem.",
  journal  = "arXiv: Optimization and Control",
  month    =  dec,
  year     =  2017
}

@ARTICLE{Bertsimas2022-qo,
  title    = "Sparse {PCA}: A geometric approach",
  author   = "Bertsimas, D and Kitane, Driss Lahlou",
  abstract = "We consider the problem of maximizing the variance explained from
              a data matrix using orthogonal sparse principal components that
              have a support of fixed cardinality. While most existing methods
              focus on building principal components (PCs) iteratively through
              deflation, we propose GeoSPCA, a novel algorithm to build all PCs
              at once while satisfying the orthogonality constraints which
              brings substantial benefits over deflation. This novel approach
              is based on the left eigenvalues of the covariance matrix which
              helps circumvent the non-convexity of the problem by
              approximating the optimal solution using a binary linear
              optimization problem that can find the optimal solution. The
              resulting approximation can be used to tackle different versions
              of the sparse PCA problem including the case in which the
              principal components share the same support or have disjoint
              supports and the Structured Sparse PCA problem. We also propose
              optimality bounds and illustrate the benefits of GeoSPCA in
              selected real world problems both in terms of explained variance,
              sparsity and tractability. Improvements vs. the greedy algorithm,
              which is often at par with state-of-the-art techniques, reaches
              up to 24\% in terms of variance while solving real world problems
              with 10,000s of variables and support cardinality of 100s in
              minutes. We also apply GeoSPCA in a face recognition problem
              yielding more than 10\% improvement vs. other PCA based technique
              such as structured sparse PCA.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  24,
  pages    = "32:1--32:33",
  month    =  oct,
  year     =  2022
}

@ARTICLE{Hastie2015-qa,
  title    = "Statistical Learning with sparsity: The lasso and Generalizations",
  author   = "Hastie, T and Tibshirani, R and Wainwright, M",
  abstract = "Discover New Methods for Dealing with High-Dimensional Data A
              sparse statistical model has only a small number of nonzero
              parameters or weights; therefore, it is much easier to estimate
              and interpret than a dense model. Statistical Learning with
              Sparsity: The Lasso and Generalizations presents methods that
              exploit sparsity to help recover the underlying signal in a set
              of data. Top experts in this rapidly evolving field, the authors
              describe the lasso for linear regression and a simple coordinate
              descent algorithm for its computation. They discuss the
              application of 1 penalties to generalized linear models and
              support vector machines, cover generalized penalties such as the
              elastic net and group lasso, and review numerical methods for
              optimization. They also present statistical inference methods for
              fitted (lasso) models, including the bootstrap, Bayesian methods,
              and recently developed approaches. In addition, the book examines
              matrix decomposition, sparse multivariate analysis, graphical
              models, and compressed sensing. It concludes with a survey of
              theoretical results for the lasso. In this age of big data, the
              number of features measured on a person or object can be large
              and might be larger than the number of observations. This book
              shows how the sparsity assumption allows us to tackle these
              problems and extract useful and reproducible patterns from big
              datasets. Data analysts, computer scientists, and theorists will
              appreciate this thorough and up-to-date treatment of sparse
              statistical modeling.",
  month    =  may,
  year     =  2015
}


@ARTICLE{Chen2001-hh,
  title    = "Atomic Decomposition by Basis Pursuit",
  author   = "{Scott Shaobing Chen and David L. Donoho and Michael A. Saunders}",
  abstract = "The time-frequency and time-scale communities have recently
              developed a large number of overcomplete waveform
              dictionaries---stationary wavelets, wavelet packets, cosine
              packets, chirplets, and warplets, to name a few. Decomposition
              into overcomplete systems is not unique, and several methods for
              decomposition have been proposed, including the method of frames
              (MOF), matching pursuit (MP), and, for special dictionaries, the
              best orthogonal basis (BOB). Basis pursuit (BP) is a principle
              for decomposing a signal into an ``optimal'' superposition of
              dictionary elements, where optimal means having the smallest l 1
              norm of coefficients among all such decompositions. We give
              examples exhibiting several advantages over MOF, MP, and BOB,
              including better sparsity and superresolution. BP has interesting
              relations to ideas in areas as diverse as ill-posed problems,
              abstract harmonic analysis, total variation denoising, and
              multiscale edge denoising. BP in highly overcomplete dictionaries
              leads to large-scale optimization problems. With signals of
              length 8192 and a wavelet packet dictionary, one gets an
              equivalent linear program of size 8192 by 212,992. Such problems
              can be attacked successfully only because of recent advances in
              linear and quadratic programming by interior-point methods. We
              obtain reasonable success with a primal-dual logarithmic barrier
              method and conjugategradient solver.",
  journal  = "SIAM REVIEW",
  volume   =  43,
  number   =  1,
  pages    = "129",
  month    =  feb,
  year     =  2001
}


@ARTICLE{Yuan2006-bt,
  title     = "Model selection and estimation in regression with grouped
               variables",
  author    = "Yuan, Ming and Lin, Yi",
  abstract  = "SummaryWe consider the problem of selecting grouped variables
               (factors) for accurate prediction in regression. Such a problem
               arises naturally in many practical situations with the
               multifactor analysis-of-variance problem as the most important
               and well-known example. Instead of selecting factors by stepwise
               backward elimination, we focus on the accuracy of estimation and
               consider extensions of the lasso, the LARS algorithm and the
               non-negative garrotte for factor selection. The lasso, the LARS
               algorithm and the non-negative garrotte are recently proposed
               regression methods that can be used to select individual
               variables. We study and propose efficient algorithms for the
               extensions of these methods for factor selection and show that
               these extensions give superior performance to the traditional
               stepwise backward elimination method in factor selection
               problems. We study the similarities and the differences between
               these methods. Simulations and real examples are used to
               illustrate the methods.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Oxford University Press (OUP)",
  volume    =  68,
  number    =  1,
  pages     = "49--67",
  month     =  feb,
  year      =  2006,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@ARTICLE{Mallat93-wi,
  author={Mallat, S.G. and Zhifeng Zhang},
  journal={IEEE Transactions on Signal Processing}, 
  title={Matching pursuits with time-frequency dictionaries}, 
  year={1993},
  volume={41},
  number={12},
  pages={3397-3415},
  keywords={Matching pursuit algorithms;Time frequency analysis;Dictionaries;Pursuit algorithms;Fourier transforms;Signal representations;Vocabulary;Signal processing algorithms;Interference;Natural languages},
  doi={10.1109/78.258082}}
  
  
@ARTICLE{Wu2019-uk,
  title         = "Recent Advances in Diversified Recommendation",
  author        = "Wu, Qiong and Liu, Yong and Miao, Chunyan and Zhao, Yin and
                   Guan, Lu and Tang, Haihong",
  abstract      = "With the rapid development of recommender systems, accuracy
                   is no longer the only golden criterion for evaluating
                   whether the recommendation results are satisfying or not. In
                   recent years, diversity has gained tremendous attention in
                   recommender systems research, which has been recognized to
                   be an important factor for improving user satisfaction. On
                   the one hand, diversified recommendation helps increase the
                   chance of answering ephemeral user needs. On the other hand,
                   diversifying recommendation results can help the business
                   improve product visibility and explore potential user
                   interests. In this paper, we are going to review the recent
                   advances in diversified recommendation. Specifically, we
                   first review the various definitions of diversity and
                   generate a taxonomy to shed light on how diversity have been
                   modeled or measured in recommender systems. After that, we
                   summarize the major optimization approaches to diversified
                   recommendation from a taxonomic view. Last but not the
                   least, we project into the future and point out trending
                   research directions on this topic.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1905.06589"
}


@ARTICLE{Wu2019-uk,
  title         = "Recent Advances in Diversified Recommendation",
  author        = "Wu, Qiong and Liu, Yong and Miao, Chunyan and Zhao, Yin and
                   Guan, Lu and Tang, Haihong",
  abstract      = "With the rapid development of recommender systems, accuracy
                   is no longer the only golden criterion for evaluating
                   whether the recommendation results are satisfying or not. In
                   recent years, diversity has gained tremendous attention in
                   recommender systems research, which has been recognized to
                   be an important factor for improving user satisfaction. On
                   the one hand, diversified recommendation helps increase the
                   chance of answering ephemeral user needs. On the other hand,
                   diversifying recommendation results can help the business
                   improve product visibility and explore potential user
                   interests. In this paper, we are going to review the recent
                   advances in diversified recommendation. Specifically, we
                   first review the various definitions of diversity and
                   generate a taxonomy to shed light on how diversity have been
                   modeled or measured in recommender systems. After that, we
                   summarize the major optimization approaches to diversified
                   recommendation from a taxonomic view. Last but not the
                   least, we project into the future and point out trending
                   research directions on this topic.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1905.06589"
}



@ARTICLE{Carbonell2017-gi,
  title     = "The use of {MMR}, diversity-based reranking for reordering
               documents and producing summaries",
  author    = "Carbonell, Jaime and Goldstein, Jade",
  abstract  = "This paper presents a method for combining query-relevance with
               information-novelty in the context of text retrieval and
               summarization. The Maximal Marginal Relevance (MMR) criterion
               strives to reduce redundancy while maintaining query relevance
               in re-ranking retrieved documents and in selecting apprw priate
               passages for text summarization. Preliminary results indicate
               some benefits for MMR diversity ranking in document retrieval
               and in single document summarization. The latter are borne out
               by the recent results of the SUMMAC conference in the evaluation
               of summarization systems. However, the clearest advantage is
               demonstrated in constructing non-redundant multi-document
               summaries, where MMR results are clearly superior to non-MMR
               passage selection.",
  journal   = "SIGIR Forum",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  51,
  number    =  2,
  pages     = "209--210",
  month     =  aug,
  year      =  1998,
  copyright = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  language  = "en"
}

@phdthesis{Koelle2022-lp,
  author       = {Samson Jonathan Koelle},
  title        = {Geometric algorithms for interpretable manifold learning},
  school       = {University of Washington},
  year         = {2022},
  type         = {PhD thesis},
  url          = {http://hdl.handle.net/1773/48559},
  abstract     = {This thesis proposes several algorithms in the area of interpretable unsupervised learning. Chapters 3 and 4 introduce a sparse convex regression approach for identifying local diffeomorphisms from a dictionary of interpretable functions. In Chapter 3, this algorithm makes use of an embedding learned by a manifold learning algorithm, while in Chapter 4, this algorithm is applied without the use of a precomputed embedding. Chapter 5 then introduces a set of alternative algorithms that avoid issues stemming from sparse regression, characterizes the tangent space version of this algorithm as identifying isometries when available, and gives a two-stage algorithm combining this approach with the computational advantages of the algorithms in Chapters 3 and 4. Finally, Chapter 6 gives an alternate tangent space estimator based on a learned embedding, and uses this as an initial estimator to tackle the related gradient estimation problem. Together, these approaches provide a toolbox of methods for computing and associating gradient information to learn descriptive parameterizations of data manifolds.},
  note         = {Statistics [108]},
  file         = {Koelle_washington_0250E_23825.pdf:22.46Mb},
}



@techreport{Tropp04-ju,
  title = {JUST RELAX: Convex Programming Methods for Subset Selection and Sparse Approximation},
  author = {Joel A. Tropp},
  year = {2004},
  institution = {Institute for Computational Engineering and Sciences, The University of Texas at Austin},
  type = {Technical Report},
  number = {04-04},
  url = {\url{https://www.oden.utexas.edu/media/reports/2004/0404.pdf}}
}

@misc{cvxpy_sparse_solution,
  title = {Sparse Solution with CVXPY},
  author = {{CVXPY Developers}},
  howpublished = {\url{https://www.cvxpy.org/examples/applications/sparse_solution.html}},
  note = {Accessed: 2024-07-11}
}


  
  @article{Tropp06-sg,
title = {Algorithms for simultaneous sparse approximation. Part II: Convex relaxation},
journal = {Signal Processing},
volume = {86},
number = {3},
pages = {589-602},
year = {2006},
note = {Sparse Approximations in Signal and Image Processing},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2005.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0165168405002239},
author = {Joel A. Tropp},
keywords = {Combinatorial optimization, Convex relaxation, Multiple measurement vectors, Simultaneous sparse approximation},
abstract = {A simultaneous sparse approximation problem requests a good approximation of several input signals at once using different linear combinations of the same elementary signals. At the same time, the problem balances the error in approximation against the total number of elementary signals that participate. These elementary signals typically model coherent structures in the input signals, and they are chosen from a large, linearly dependent collection. The first part of this paper presents theoretical and numerical results for a greedy pursuit algorithm, called simultaneous orthogonal matching pursuit. The second part of the paper develops another algorithmic approach called convex relaxation. This method replaces the combinatorial simultaneous sparse approximation problem with a closely related convex program that can be solved efficiently with standard mathematical programming software. The paper develops conditions under which convex relaxation computes good solutions to simultaneous sparse approximation problems.}
}

@INPROCEEDINGS{Tropp05-ml,
  author={Tropp, J.A. and Gilbert, A.C. and Strauss, M.J.},
  booktitle={Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.}, 
  title={Simultaneous sparse approximation via greedy pursuit}, 
  year={2005},
  volume={5},
  number={},
  pages={v/721-v/724 Vol. 5},
  keywords={Dictionaries;Sparse matrices;Matching pursuit algorithms;Optimized production technology;Mathematics;Vectors;Signal analysis;Pursuit algorithms;Approximation algorithms;Approximation error},
  doi={10.1109/ICASSP.2005.1416405}}
  
@BOOK{James2014-eg,
  title     = "An Introduction to Statistical Learning: with Applications in
               {R}",
  author    = "James, Gareth and Witten, Daniela and Hastie, Trevor and
               Tibshirani, Robert",
  abstract  = "An Introduction to Statistical Learning provides an accessible
               overview of the field of statistical learning, an essential
               toolset for making sense of the vast and complex data sets that
               have emerged in fields ranging from biology to finance to
               marketing to astrophysics in the past twenty years. This book
               presents some of the most important modeling and prediction
               techniques, along with relevant applications. Topics include
               linear regression, classification, resampling methods, shrinkage
               approaches, tree-based methods, support vector machines,
               clustering, and more. Color graphics and real-world examples are
               used to illustrate the methods presented. Since the goal of this
               textbook is to facilitate the use of these statistical learning
               techniques by practitioners in science, industry, and other
               fields, each chapter contains a tutorial on implementing the
               analyses and methods presented in R, an extremely popular open
               source statistical software platform.Two of the authors co-wrote
               The Elements of Statistical Learning (Hastie, Tibshirani and
               Friedman, 2nd edition 2009), a popular reference book for
               statistics and machine learning researchers. An Introduction to
               Statistical Learning covers many of the same topics, but at a
               level accessible to a much broader audience. This book is
               targeted at statisticians and non-statisticians alike who wish
               to use cutting-edge statistical learning techniques to analyze
               their data. The text assumes only a previous course in linear
               regression and no knowledge of matrix algebra.",
  publisher = "Springer New York",
  month     =  jul,
  year      =  2014,
  language  = "en"
}


@ARTICLE{Obozinski2006-kq,
  title    = "Multi-task feature selection",
  author   = "Obozinski, G and Taskar, B and Jordan, Michael I",
  abstract = "We address the problem of joint feature selection across a group
              of related classification or regression tasks. We propose a novel
              type of joint regularization of the model parameters in order to
              couple feature selection across tasks. Intuitively, we extend the
              `1 regularization for single-task estimation to the multi-task
              setting. By penalizing the sum of `2-norms of the blocks of
              coefficients associated with each feature across different tasks,
              we encourage multiple predictors to have similar parameter
              sparsity patterns. To fit parameters under this regularization,
              we propose a blockwise boosting scheme that follows the
              regularization path. The algorithm introduces and updates
              simultaneously the coefficients associated with one feature in
              all tasks. We show empirically that this approach outperforms
              independent `1-based feature selection on several datasets.",
  year     =  2006
}


@ARTICLE{Yeung2011-fg,
  title    = "A probabilistic framework for learning task relationships in
              multi-task learning",
  author   = "Yeung, Dit-Yan and Zhang, Yu",
  abstract = "For many real-world machine learning applications, labeled data
              is costly because the data labeling process is laborious and time
              consuming. As a consequence, only limited labeled data is
              available for model training, leading to the so-called labeled
              data deficiency problem. In the machine learning research
              community, several directions have been pursued to address this
              problem. Among these efforts, a promising direction is multi-task
              learning which is a learning paradigm that seeks to boost the
              generalization performance of a model on a learning task with the
              help of some other related tasks. This learning paradigm has been
              inspired by human learning activities in that people often apply
              the knowledge gained from previous learning tasks to help learn a
              new task more efficiently and effectively. Of the several
              approaches proposed in previous research for multi-task learning,
              a relatively less studied yet very promising approach is based on
              automatically learning the relationships among tasks from data.
              In this thesis, we first propose a powerful probabilistic
              framework for multi-task learning based on the task relationship
              learning approach. The main novelty of our framework lies in the
              use of a matrix variate prior with parameters that model task
              relationships. Based on this general multi-task learning
              framework, we then propose four specific methods, namely,
              multi-task relationship learning (MTRL), multi-task generalized t
              process (MTGTP), multi-task high-order task relationship learning
              (MTHOL), and probabilistic multi-task feature selection (PMTFS).
              By utilizing a matrix variate normal distribution as a prior on
              the model parameters of all tasks, MTRL can be formulated
              efficiently as a convex optimization problem. On the other hand,
              MTGTP is a Bayesian method that models the task covariance matrix
              as a random matrix with an inverse-Wishart prior and integrates
              it out to achieve Bayesian model averaging to improve the
              generalization performance. With MTRL as a base, MTHOL provides a
              generalization that learns high-order task relationships and
              model parameters. Unlike MTRL, MTGTP and MTHOL which are for
              standard multi-task classification or regression problems, PMTFS
              addresses the feature selection problem under the multi-task
              setting by incorporating the learning of task relationships.
              Besides conducting experimental validation of the proposed
              methods on several data sets for multi-task learning, we also
              investigate in detail a collaborative filtering application under
              the multi-task setting. Through both theoretical and empirical
              studies on the several methods proposed, we show that task
              relationship learning is a very promising approach for multi-task
              learning and related learning problems.",
  year     =  2011
}


@article{Friedman-2007-yb,
author = {Jerome Friedman and Trevor Hastie and Holger H{\"o}fling and Robert Tibshirani},
title = {{Pathwise coordinate optimization}},
volume = {1},
journal = {The Annals of Applied Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {302 -- 332},
keywords = {Convex optimization, Coordinate descent, Lasso},
year = {2007},
doi = {10.1214/07-AOAS131},
URL = {https://doi.org/10.1214/07-AOAS131}
}


@ARTICLE{Liu2009-yo,
  title    = "Blockwise coordinate descent procedures for the multi-task lasso,
              with applications to neural semantic basis discovery",
  author   = "Liu, Han and Palatucci, Mark and Zhang, Jian",
  abstract = "We develop a cyclical blockwise coordinate descent algorithm for
              the multi-task Lasso that efficiently solves problems with
              thousands of features and tasks. The main result shows that a
              closed-form Winsorization operator can be obtained for the
              sup-norm penalized least squares regression. This allows the
              algorithm to find solutions to very large-scale problems far more
              efficiently than existing methods. This result complements the
              pioneering work of Friedman, et al. (2007) for the single-task
              Lasso. As a case study, we use the multi-task Lasso as a variable
              selector to discover a semantic basis for predicting human neural
              activation. The learned solution outperforms the standard basis
              for this task on the majority of test participants, while
              requiring far fewer assumptions about cognitive neuroscience. We
              demonstrate how this learned basis can yield insights into how
              the brain represents the meanings of words.",
  journal  = "ICML",
  pages    = "649--656",
  month    =  jun,
  year     =  2009
}



% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Catalina2018-ek,
  title    = "Revisiting {FISTA} for Lasso: Acceleration strategies over the
              regularization path",
  author   = "Catalina, Alejandro and Ala{\'\i}z, Carlos M and Dorronsoro,
              Jos{\'e} R",
  abstract = ". In this work we revisit FISTA algorithm for Lasso showing that
              recent acceleration techniques may greatly improve its basic
              version, resulting in a much more competitive procedure. We study
              the contribution of the diﬀerent improvement strategies, showing
              experimentally that the ﬁnal version becomes much faster than the
              standard one.",
  journal  = "Eur Symp Artif Neural Netw",
  year     =  2018
}


@ARTICLE{Sun2012-vp,
  title         = "Sparse Matrix Inversion with Scaled Lasso",
  author        = "Sun, Tingni and Zhang, Cun-Hui",
  abstract      = "We propose a new method of learning a sparse
                   nonnegative-definite target matrix. Our primary example of
                   the target matrix is the inverse of a population covariance
                   or correlation matrix. The algorithm first estimates each
                   column of the target matrix by the scaled Lasso and then
                   adjusts the matrix estimator to be symmetric. The penalty
                   level of the scaled Lasso for each column is completely
                   determined by data via convex minimization, without using
                   cross-validation. We prove that this scaled Lasso method
                   guarantees the fastest proven rate of convergence in the
                   spectrum norm under conditions of weaker form than those in
                   the existing analyses of other $\ell_1$ regularized
                   algorithms, and has faster guaranteed rate of convergence
                   when the ratio of the $\ell_1$ and spectrum norms of the
                   target inverse matrix diverges to infinity. A simulation
                   study demonstrates the computational feasibility and superb
                   performance of the proposed method. Our analysis also
                   provides new performance bounds for the Lasso and scaled
                   Lasso to guarantee higher concentration of the error at a
                   smaller threshold level than previous analyses, and to allow
                   the use of the union bound in column-by-column applications
                   of the scaled Lasso without an adjustment of the penalty
                   level. In addition, the least squares estimation after the
                   scaled Lasso selection is considered and proven to guarantee
                   performance bounds similar to that of the scaled Lasso.",
  month         =  feb,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1202.2723"
}


@ARTICLE{Jones2007-uc,
  title         = "Universal local parametrizations via heat kernels and
                   eigenfunctions of the Laplacian",
  author        = "Jones, Peter W and Maggioni, Mauro and Schul, Raanan",
  abstract      = "We use heat kernels or eigenfunctions of the Laplacian to
                   construct local coordinates on large classes of Euclidean
                   domains and Riemannian manifolds (not necessarily smooth,
                   e.g. with $\mathcal\{C\}^\alpha$ metric). These coordinates
                   are bi-Lipschitz on embedded balls of the domain or
                   manifold, with distortion constants that depend only on
                   natural geometric properties of the domain or manifold. The
                   proof of these results relies on estimates, from above and
                   below, for the heat kernel and its gradient, as well as for
                   the eigenfunctions of the Laplacian and their gradient.
                   These estimates hold in the non-smooth category, and are
                   stable with respect to perturbations within this category.
                   Finally, these coordinate systems are intrinsic and
                   efficiently computable, and are of value in applications.",
  month         =  sep,
  year          =  2007,
  archivePrefix = "arXiv",
  primaryClass  = "math.AP",
  eprint        = "0709.1975"
}


@ARTICLE{Chen2019-km,
  title    = "Selecting the independent coordinates of manifolds with large
              aspect ratios",
  author   = "Chen, Yu-Chia and Meil{\u a}, M",
  abstract = "Many manifold embedding algorithms fail apparently when the data
              manifold has a large aspect ratio (such as a long, thin strip).
              Here, we formulate success and failure in terms of finding a
              smooth embedding, showing also that the problem is pervasive and
              more complex than previously recognized. Mathematically, success
              is possible under very broad conditions, provided that embedding
              is done by carefully selected eigenfunctions of the
              Laplace-Beltrami operator $\Delta$. Hence, we propose a
              bicriterial Independent Eigencoordinate Selection (IES) algorithm
              that selects smooth embeddings with few eigenvectors. The
              algorithm is grounded in theory, has low computational overhead,
              and is successful on synthetic and large real data.",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   = "abs/1907.01651",
  month    =  jul,
  year     =  2019
}

@inproceedings{He2023-ch,
  title     = "Product Manifold Learning with Independent Coordinate Selection",
  author    = "He, Jesse and Brug{\`e}re, Tristan and Mishne, Gal",
  booktitle = "Proceedings of the 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at ICML",
  abstract  = "In many dimensionality reduction tasks, we wish to identify the
               constituent components that explain our observations. For
               manifold learning, this can be formalized as factoring a
               Riemannian product manifold. Recovering this factorization,
               however, may suffer from certain difficulties in practice,
               especially when data is sparse or noisy, or when one factor is
               distorted by the other. To address these limitations, we propose
               identifying non-redundant coordinates on the product manifold
               before applying product manifold learning to identify which
               coordinates correspond to different factor manifolds. We
               demonstrate our approach on both synthetic and real-world data.",
  month     = jun,
  year      = 2023
}


@INPROCEEDINGS{Koelle2024-no,
  title     = "Consistency of dictionary-based manifold learning",
  booktitle = "Proceedings of The 27th International Conference on Artificial
               Intelligence and Statistics",
  author    = "Koelle, Samson J and Zhang, Hanyu and Murad, Octavian-Vlad and
               Meila, Marina",
  editor    = "Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen",
  abstract  = "We analyze a paradigm for interpretable Manifold Learning for
               scientific data analysis, whereby one parametrizes a manifold
               with d smooth functions from a scientist-provided dictionary of
               meaningful, domain-related functions. When such a
               parametrization exists, we provide an algorithm for finding it
               based on sparse regression in the manifold tangent bundle,
               bypassing more standard, agnostic manifold learning algorithms.
               We prove conditions for the existence of such parameterizations
               in function space and the first end to end recovery results from
               finite samples. The method is demonstrated on both synthetic
               problems and with data from a real scientific domain.",
  publisher = "PMLR",
  volume    =  238,
  pages     = "4348--4356",
  series    = "Proceedings of Machine Learning Research",
  year      =  2024
}



@ARTICLE{Koelle2022-ju,
  title    = "Manifold Coordinates with Physical Meaning",
  author   = "Koelle, Samson J and Zhang, Hanyu and Meila, Marina and Chen,
              Yu-Chia",
  journal  = "J. Mach. Learn. Res.",
  volume   =  23,
  number   =  133,
  pages    = "1--57",
  year     =  2022
}

    @article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }
