\section{Discussion}
\label{sec:discussion}

We have shown the efficacy of a convex multitask basis pursuit approach for selecting isometric submatrices from wide-matrices.
This approach - \isometrypursuit~ - is alternative to greedy methods for selection of orthonormalized features from within a dictionary.
It can be applied to diversification and geometrically-faithful coordinate estimation.
Compared with Sparse PCA \citep{Dey2017-mx, Bertsimas2022-qo, Bertsimas2022-dv}, we seek a low-dimensional representation from the set of column vectors rather than their span.

Algorithmic variants of interest include the multitask lasso extension of our estimator.
Applications of interest include greedy diversification in recommendation systems \cite{Carbonell2017-gi, Wu2019-uk} and other retrieval-type systems, as well as decomposing interpretable yet overcomplete dictionaries in transformer residual streams.
The most pressing piece of remaining theoretical work removal of the restriction $|S| = D$ on the conditions of Proposition \ref{prop:unitary_selection} and investigating of the necessity of the second selection step.
The resulting proposition is intuitive but is more difficult to prove and seemingly violated by (omitted) empirical results that are subtly non-dispositive in the sense that improvements of primal loss below $D$ are accompanied by violations of the constraint of a similar magnitude.
We also note that isometries may not always exist in the presence of curvature, so comparison of our loss with curvature could prove fertile, as could comparison with the so-called restricted isometry property used to show guaranteed recovery at fast convergence rates in supervised learning \cite{Candes2005-dd, Hastie2015-qa}.

% Multitask learning and lasso \cite{Hastie2015-qa}
%The comparison of greedy (e.g. Orthogonal Matching Pursuit) \cite{Mallat93-wi, Tropp05-ml} and convex \cite{Tropp06-sg} basis pursuit formulations.