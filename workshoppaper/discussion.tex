\section{Discussion}
\label{sec:discussion}

We have shown that multitask basis pursuit can help select isometric submatrices from appropriately normalized wide-matrices.
This approach - \isometrypursuit~ - is convex alternative to greedy methods for selection of orthonormalized features from within a dictionary.
\isometrypursuit~ can be applied to diversification and geometrically-faithful coordinate estimation.
Applications of interest include diversification in recommendation systems \cite{Carbonell2017-gi, Wu2019-uk, Langchain} and other retrieval systems such as in RAG \cite{Gao2023-cn, Pickett2024-ad, In2024-um, Weiss2024-xm, Vectara}, as well as decomposing interpretable yet overcomplete dictionaries in transformer residual streams.

Compared with the greedy algorithms used in such areas \cite{Carbonell1998-ji, Barioni, Drosou, Qin2012-ok, KUNAVER2017154, Guo-shengbo, Abdool,Yu2016AGA,  Huang2024-wr, Pickett2024-ad}, the convex reformulation may add speed and convergence to a global minima.
The comparison of greedy \cite{Mallat93-wi, Mallat, Pati-93, Tropp05-ml} and convex \cite{Chen2001-hh, Tropp06-sg,Chen2006TheoreticalRO} basis pursuit formulations has a rich history, and theoretical understanding of the behavior of this approximation is evolving.
Diversification problems have been cited as NP-hard, and isometry pursuit can be considered analogous to them in the sense of basis pursuit and the lasso against best subset selection, with the caveat that best subset selection of the basis pursuit loss minimizer isn't totally equivalent to isometry pursuit even though they share the same unique optimum.
Characterization of solutions resulting from removal of the restriction $|S| = D$ on the conditions of Proposition \ref{prop:unitary_selection} may help justify the second selection step.
That the solution of a lasso problem can sometimes be a non-singleton convex set containing the $\ell 0 $ solution is well-known \cite{Osborne2000OnTL, DOSSAL2012117, Chrtien2011OnTG, Tibshirani2012TheLP, Ewald2017OnTD, Ali2018TheGL, Schneider2020-qt, Mishkin2022TheSP,Dupuis2019TheGO,Debarre2020OnTU,Everink2024TheGA}, and it appears empirically in our situation that this can occur even when the design matrix is not in so-called general position.
The best subset support estimate - the isometry estimate - may be contained within the loss preimage fiber at the minimum loss.
The convergence of SCS algorithm to the 2 norm minimizing solution due to the dual constraint penalty and the convexity of this premise may then imply that the two stage procedure would then always succeed.
Related conditions have been discussed in \cite{Donoho2006ForML, Mishkin2022TheSP}, and we examine this topic experimentally in Section \ref{sec:deep_dive}.

Algorithmic variants include the multitask lasso \cite{ Hastie2015-qa} extension of our estimator.
Tangent-space specific variants have been studied in more detail in \cite{Koelle2022-ju, Koelle2024-no} with additional grouping across datapoints, and a corresponding variant of the isometry theorem that missed non-uniqueness was claimed in \cite{Koelle2022-lp}.
Comparison of our loss with curvature - whose presence prohibits $D$ element isometry - could prove fertile, as could comparison with the so-called restricted isometry property used to show guaranteed recovery at fast convergence rates in supervised learning \cite{Candes2005-dd, Hastie2015-qa}.