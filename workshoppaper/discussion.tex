\section{Discussion}
\label{sec:discussion}

This extension is a local version of Tangent Space Lasso.
%The Hoeffding bound
%Dimension estimation, the failure of duality
%The presence of curvature

Tangent space basis pursuit satisfies a similar property \cite{Koelle2022-lp} but the normalization process differs.

It could be used in the stiching step of an algorithm like the kohli one
We leave aside the question of patch alignment \cite{https://arxiv.org/pdf/2303.11620.pdf, LDLE paper}.
The full gradient approach.
In this case normalization prior to projection is subsumbmed by the larger coefficients needed to get the tangent space.
Good news is tangent space estimation need not be performed.
Let's compare the coefficients involved in projecting versus not projecting.
We can perform regression in the high dimensional space instead of projecting on span of target variable.

With respect to pseudoinverse estimation, sparse methods have been applied in \cite{Sun2012-vp}

Even though by Lagrangian duality, the basis pursuit solution corresponds to $\lambda$ approaching $0$, the solution is sparse \cite{Tropp04-ju}.
about the lasso is that all coefficients enter the regularization path.
As we see by the correspondence between $\lambda$ approaching $0$ and the basis pursuit problem, some coefficients in fact do not go to $0$. 


%One immediate question is - why $\beta$?  Can we just minimize some function of $X$ directly like $\|\exp_1 X\|_{1,2}$?  Direct minimization of this norm has nothing to do with correlation. We wouldn't need orthogonality - just constant length!



% Proof of isometry (Copy from thesis)

Proof of local isometry (simpler proof since no oscillation game)

% failure of duality

\cite{Bertsimas2022-qo} gives a method for solving the sparse-PCA method more efficiently than the original greedy approach.
Compared with the FISTA method used in  \cite{Koelle2022-ju, Koelle2024-no}, coordinate descent \cite{Friedman-2007-yb, Meier2008-ts, Qin2013-tx} is faster \cite{Catalina2018-ek, Zhao2023-xn}.
Compared with \cite{Liu2009-yo}, the sklearn multitask lasso is $2,1$ rather than $\infty,1$ regularized.

Compared with Gram-Schmidt
It is likely that the transformed singular value loss could be reframed as a semdefinite programming problem, since the composition of two convex functions is convex \cite{Boyd2004-ql}.
%\section{Comparison with other approach}

Multitask lasso \cite{Obozinski2006-kq, Yeung2011-fg} is a form of group lasso \cite{Yuan2006-bt} where coefficients are group by response variable.

See \cite{Obozinski2006-kq} for a comparison of forward and backward selection with lasso.
%\subsection{Projection approach}

Our notion of isometric recovery is distinct from the restricted isometry property \cite{Candes2005-dd, Hastie2015-qa}, which is used to show guaranteed recovery at fast convergence rates in supervised learning.
In particular, our approach does not consider statistical error or the presence of a true underlying model.
However, we note that disintigration of performance at high $\lambda$ values in the lasso formulation may have some relation to these properties, as discussed in \cite{Koelle2022-ju, Koelle2024-no}.
%\begin{align}
%x_M = (y^T y) x \\ 
%({x_M}^T {x_M})^{-1} {x_M} I = ({(y^T y x)}^T {(y^T y x)})^{-1} {(y^T y x)} I = (x^T y^T y x)^{-1} (y^T y x)
%\end{align}

%low distortion just means conformal?

%Other interesting open questions about basis pursuit that are not covered here include can we equate the intrinsic curvature of the data manifold to the basis pursuit loss, or what happens when dictionary = feature space, or what happens if we estimate in the coordinates of the ambient space, or what are the convergence rates.  Convergence rate related to tangent space estimation.

% singular value criteria could be directly optimized.  singular value criteria would not be sparse
% recommendation systems.  
% RAG

A major area of comparison is in diversification in recommendation systems.  Greedy algorithms are used \cite{Carbonell2017-gi, Wu2019-uk}

Compared with sparse pca \cite{Bertsimas2022-qo, Bertsimas2022-dv}, we are not concerned with variability in the dataset, and select.
While the sparse PCA problem is non-convex, our approach can be taken as a simpler version in the sense that the loadings are constrained to be the identify matrix.
\cite{Tropp06-sg} and \cite{Liu2009-yo} use a $1,\infty$ norm to induce sparsity that misses the utility of our normalization for finding unitary matrices.
since isometry embeddings preserve important properties like distances between points.

