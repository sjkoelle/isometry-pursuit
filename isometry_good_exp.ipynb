{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b813b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import montlake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ceb7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package montlake:\n",
      "\n",
      "NAME\n",
      "    montlake\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    None\n",
      "    _nbdev\n",
      "    atomgeom (package)\n",
      "    core\n",
      "    exec (package)\n",
      "    geometry (package)\n",
      "    gradients (package)\n",
      "    mflasso (package)\n",
      "    optimization (package)\n",
      "    plotting (package)\n",
      "    simulations (package)\n",
      "    statistics (package)\n",
      "    tslasso (package)\n",
      "    utils (package)\n",
      "    vendor (package)\n",
      "\n",
      "VERSION\n",
      "    0.0.1\n",
      "\n",
      "FILE\n",
      "    /Users/samsonkoelle/miniconda3/envs/montlake_xxx/lib/python3.6/site-packages/montlake/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(montlake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c908161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/optimization.gradientgrouplasso.ipynb (unless otherwise specified).\n",
    "\n",
    "__all__ = ['GradientGroupLasso', 'get_sr_lambda_parallel', 'batch_stream']\n",
    "\n",
    "# Cell\n",
    "#loosely inspired by the pyglmnet package\n",
    "from einops import rearrange\n",
    "#import autograd.numpy as np\n",
    "import numpy as np\n",
    "\n",
    "class GradientGroupLasso:\n",
    "\n",
    "    def __init__(self, dg_M, df_M, reg_l1s, reg_l2, max_iter,learning_rate, tol, beta0_npm= None):\n",
    "\n",
    "        n = dg_M.shape[0]\n",
    "        d= dg_M.shape[1]\n",
    "        m = df_M.shape[2]\n",
    "        p = dg_M.shape[2]\n",
    "        dummy_beta = np.ones((n,p,m))\n",
    "\n",
    "        self.dg_M = dg_M\n",
    "        self.df_M = df_M\n",
    "        self.reg_l1s = reg_l1s\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.beta0_npm = beta0_npm\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.m = m\n",
    "        self.d = d\n",
    "        self.dummy_beta = dummy_beta\n",
    "        #self.group = np.asarray(group)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "\n",
    "    def _prox(self,beta_npm, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "\n",
    "        p = self.p\n",
    "        result = np.zeros(beta_npm.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        for j in range(p):\n",
    "            if np.linalg.norm(beta_npm[:,j,:]) > 0.:\n",
    "                potentialoutput = beta_npm[:,j,:] - (thresh / np.linalg.norm(beta_npm[:,j,:])) * beta_npm[:,j,:]\n",
    "                posind = np.asarray(np.where(beta_npm[:,j,:] > 0.))\n",
    "                negind = np.asarray(np.where(beta_npm[:,j,:] < 0.))\n",
    "                po = beta_npm[:,j,:].copy()\n",
    "                po[posind[0],posind[1]] = np.asarray(np.clip(potentialoutput[posind[0],posind[1]],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind[0],negind[1]] = np.asarray(np.clip(potentialoutput[negind[0],negind[1]],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[:,j,:] = po\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta_npm):\n",
    "\n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "        reg_l2 = self.reg_l2\n",
    "        dummy_beta = self.dummy_beta\n",
    "\n",
    "        df_M_hat = np.einsum('ndp,npm->ndm',dg_M, beta_npm)\n",
    "        error = df_M_hat - df_M\n",
    "        grad_beta = np.einsum('ndm,ndp->npm',error,dg_M) #+ reg_l2 * np.ones()\n",
    "        #if\n",
    "        return grad_beta\n",
    "\n",
    "    def _L1penalty(self, beta_npm):\n",
    "\n",
    "        p = self.p\n",
    "        m = self.m\n",
    "        n = self.n\n",
    "        beta_mn_p = rearrange(beta_npm, 'n p m -> (m n) p')#np.reshape(beta_mnp, ((m*n,p)))\n",
    "        L1penalty = np.linalg.norm(beta_mn_p, axis = 0).sum()\n",
    "\n",
    "        return L1penalty\n",
    "\n",
    "    def _loss(self,beta_npm, reg_lambda):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta_npm)\n",
    "        P = self._L1penalty(beta_npm)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "\n",
    "    def _logL(self,beta_npm):\n",
    "\n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "\n",
    "        df_M_hat = np.einsum('ndp,npm -> ndm',dg_M, beta_npm)\n",
    "        logL = -0.5 * np.linalg.norm((df_M - df_M_hat))**2\n",
    "        return(logL)\n",
    "\n",
    "    def _L2loss(self,beta_npm):\n",
    "        output = -self._logL(beta_npm)\n",
    "        return(output)\n",
    "\n",
    "    def fhatlambda(self,learning_rate,beta_npm_new,beta_npm_old):\n",
    "\n",
    "        #print('lr',learning_rate)\n",
    "        output = self._L2loss(beta_npm_old) + np.einsum('npm,npm', self._grad_L2loss(beta_npm_old),(beta_npm_new-beta_npm_old)) + (1/(2*learning_rate)) * np.linalg.norm(beta_npm_new-beta_npm_old)**2\n",
    "\n",
    "        return(output)\n",
    "\n",
    "    def _btalgorithm(self,beta_npm ,learning_rate,b,maxiter_bt,rl):\n",
    "\n",
    "        grad_beta = self._grad_L2loss(beta_npm = beta_npm)\n",
    "        for i in range(maxiter_bt):\n",
    "            beta_npm_postgrad = beta_npm - learning_rate * grad_beta\n",
    "            beta_npm_postgrad_postprox = self._prox(beta_npm_postgrad, learning_rate * rl)\n",
    "            fz = self._L2loss(beta_npm_postgrad_postprox)\n",
    "            #fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm_postgrad)\n",
    "            fhatz = self.fhatlambda(learning_rate,beta_npm_postgrad_postprox, beta_npm)\n",
    "            if fz <= fhatz:\n",
    "                #print(i)\n",
    "                break\n",
    "            learning_rate = b*learning_rate\n",
    "\n",
    "        return(beta_npm_postgrad_postprox,learning_rate)\n",
    "\n",
    "    def fit(self, beta0_npm = None):\n",
    "\n",
    "        reg_l1s = self.reg_l1s\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        p = self.p\n",
    "\n",
    "        dg_M = self.dg_M\n",
    "        df_M = self.df_M\n",
    "\n",
    "        tol = self.tol\n",
    "        np.random.RandomState(0)\n",
    "\n",
    "        if beta0_npm is None:\n",
    "            beta_npm_hat = 1 / (n*m*p) * np.random.normal(0.0, 1.0, [n, p,m])\n",
    "            #1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        else:\n",
    "            beta_npm_hat = beta0_npm\n",
    "\n",
    "        fit_params = list()\n",
    "        for l, rl in enumerate(reg_l1s):\n",
    "            fit_params.append({'beta': beta_npm_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_npm_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "\n",
    "            alpha = 1.\n",
    "            beta_npm_hat = fit_params[-1]['beta']\n",
    "            #g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            learning_rate = self.learning_rate\n",
    "            beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "            beta_npm_hat_2 = beta_npm_hat.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                #print(t,l,rl)\n",
    "                #print(t)\n",
    "                L.append(self._loss(beta_npm_hat, rl))\n",
    "                L2.append(self._L2loss(beta_npm_hat))\n",
    "                PEN.append(self._L1penalty(beta_npm_hat))\n",
    "                w = (t / (t+ 3))\n",
    "                beta_npm_hat_momentumguess = beta_npm_hat + w*(beta_npm_hat_1 - beta_npm_hat_2)\n",
    "\n",
    "                beta_npm_hat , learning_rate = self._btalgorithm(beta_npm_hat_momentumguess,learning_rate,.5,1000, rl)\n",
    "                #print(beta_npm_hat_momentumguess.max(), beta_npm_hat.max(),self._L2loss(beta_npm_hat), learning_rate)\n",
    "                beta_npm_hat_2 = beta_npm_hat_1.copy()\n",
    "                beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "\n",
    "            fit_params[-1]['beta'] = beta_npm_hat\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "\n",
    "        self.fit_ = fit_params\n",
    "        #self.ynull_ = np.mean(y)\n",
    "\n",
    "        return self\n",
    "\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89c88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function multivariate_normal:\n",
      "\n",
      "multivariate_normal(...) method of numpy.random.mtrand.RandomState instance\n",
      "    multivariate_normal(mean, cov, size=None, check_valid='warn', tol=1e-8)\n",
      "    \n",
      "    Draw random samples from a multivariate normal distribution.\n",
      "    \n",
      "    The multivariate normal, multinormal or Gaussian distribution is a\n",
      "    generalization of the one-dimensional normal distribution to higher\n",
      "    dimensions.  Such a distribution is specified by its mean and\n",
      "    covariance matrix.  These parameters are analogous to the mean\n",
      "    (average or \"center\") and variance (standard deviation, or \"width,\"\n",
      "    squared) of the one-dimensional normal distribution.\n",
      "    \n",
      "    .. note::\n",
      "        New code should use the ``multivariate_normal`` method of a ``default_rng()``\n",
      "        instance instead; please see the :ref:`random-quick-start`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    mean : 1-D array_like, of length N\n",
      "        Mean of the N-dimensional distribution.\n",
      "    cov : 2-D array_like, of shape (N, N)\n",
      "        Covariance matrix of the distribution. It must be symmetric and\n",
      "        positive-semidefinite for proper sampling.\n",
      "    size : int or tuple of ints, optional\n",
      "        Given a shape of, for example, ``(m,n,k)``, ``m*n*k`` samples are\n",
      "        generated, and packed in an `m`-by-`n`-by-`k` arrangement.  Because\n",
      "        each sample is `N`-dimensional, the output shape is ``(m,n,k,N)``.\n",
      "        If no shape is specified, a single (`N`-D) sample is returned.\n",
      "    check_valid : { 'warn', 'raise', 'ignore' }, optional\n",
      "        Behavior when the covariance matrix is not positive semidefinite.\n",
      "    tol : float, optional\n",
      "        Tolerance when checking the singular values in covariance matrix.\n",
      "        cov is cast to double before the check.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        The drawn samples, of shape *size*, if that was provided.  If not,\n",
      "        the shape is ``(N,)``.\n",
      "    \n",
      "        In other words, each entry ``out[i,j,...,:]`` is an N-dimensional\n",
      "        value drawn from the distribution.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Generator.multivariate_normal: which should be used for new code.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The mean is a coordinate in N-dimensional space, which represents the\n",
      "    location where samples are most likely to be generated.  This is\n",
      "    analogous to the peak of the bell curve for the one-dimensional or\n",
      "    univariate normal distribution.\n",
      "    \n",
      "    Covariance indicates the level to which two variables vary together.\n",
      "    From the multivariate normal distribution, we draw N-dimensional\n",
      "    samples, :math:`X = [x_1, x_2, ... x_N]`.  The covariance matrix\n",
      "    element :math:`C_{ij}` is the covariance of :math:`x_i` and :math:`x_j`.\n",
      "    The element :math:`C_{ii}` is the variance of :math:`x_i` (i.e. its\n",
      "    \"spread\").\n",
      "    \n",
      "    Instead of specifying the full covariance matrix, popular\n",
      "    approximations include:\n",
      "    \n",
      "      - Spherical covariance (`cov` is a multiple of the identity matrix)\n",
      "      - Diagonal covariance (`cov` has non-negative elements, and only on\n",
      "        the diagonal)\n",
      "    \n",
      "    This geometrical property can be seen in two dimensions by plotting\n",
      "    generated data-points:\n",
      "    \n",
      "    >>> mean = [0, 0]\n",
      "    >>> cov = [[1, 0], [0, 100]]  # diagonal covariance\n",
      "    \n",
      "    Diagonal covariance means that points are oriented along x or y-axis:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
      "    >>> plt.plot(x, y, 'x')\n",
      "    >>> plt.axis('equal')\n",
      "    >>> plt.show()\n",
      "    \n",
      "    Note that the covariance matrix must be positive semidefinite (a.k.a.\n",
      "    nonnegative-definite). Otherwise, the behavior of this method is\n",
      "    undefined and backwards compatibility is not guaranteed.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Papoulis, A., \"Probability, Random Variables, and Stochastic\n",
      "           Processes,\" 3rd ed., New York: McGraw-Hill, 1991.\n",
      "    .. [2] Duda, R. O., Hart, P. E., and Stork, D. G., \"Pattern\n",
      "           Classification,\" 2nd ed., New York: Wiley, 2001.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> mean = (1, 2)\n",
      "    >>> cov = [[1, 0], [0, 1]]\n",
      "    >>> x = np.random.multivariate_normal(mean, cov, (3, 3))\n",
      "    >>> x.shape\n",
      "    (3, 3, 2)\n",
      "    \n",
      "    The following is probably true, given that 0.6 is roughly twice the\n",
      "    standard deviation:\n",
      "    \n",
      "    >>> list((x[0,0,:] - mean) < 0.6)\n",
      "    [True, True] # random\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "help(np.random.multivariate_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f975dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "d = 3\n",
    "p = 10\n",
    "\n",
    "sample_grads = .1* np.random.multivariate_normal(np.zeros(d), np.identity(d),p)\n",
    "\n",
    "dg_M = np.zeros((n,d ,p))\n",
    "dg_M[0] = rearrange(sample_grads, 'p d -> d p')\n",
    "df_M = np.zeros((n,d ,d))\n",
    "df_M[0] = np.identity(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f288785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_grads = .1* np.random.multivariate_normal(np.zeros(2), np.identity(2),10)\n",
    "# n = 2\n",
    "# d = 2\n",
    "# p = 10\n",
    "# dg_M = np.zeros((n,d ,p))\n",
    "# dg_M[0] = rearrange(sample_grads, 'p d -> d p')\n",
    "# sample_grads = .1* np.random.multivariate_normal(np.zeros(2), np.identity(2),10)\n",
    "# dg_M[1] = rearrange(sample_grads, 'p d -> d p')\n",
    "\n",
    "# df_M = np.zeros((n,d ,d))\n",
    "# df_M[0] = np.identity(2)\n",
    "# df_M[1] = np.identity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c735aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_itermax = 100000\n",
    "tol = 1e-16\n",
    "learning_rate = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6e4dbb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientGroupLasso at 0x7fe6e1176710>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL = GradientGroupLasso(dg_M, df_M, np.asarray([.00001]), 0., gl_itermax,learning_rate, tol, beta0_npm= None)\n",
    "GGL.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fd0324c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldnt we just keep the longest most orthogonal pair?  No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c0bedff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like maybe it works...\n",
    "# before we could see that the dual problem minimizer was actually not the cardinality 2 solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a676ac3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 3)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL.fit_[0]['beta'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e287f6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.99957330e-01,  8.88559915e-06, -1.62339409e-05],\n",
       "        [ 8.88533368e-06,  9.99959932e-01,  1.51365494e-05],\n",
       "        [-1.62332078e-05,  1.51365592e-05,  9.99953326e-01]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.einsum('i p d, i e p -> i d e',GGL.fit_[0]['beta'], dg_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b388bfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.940365223953933"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL._L1penalty(GGL.fit_[0]['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3a0ebab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.77091266,  1.42529816, -2.37471106],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 1.25847749, -1.90490518,  0.860803  ],\n",
       "        [ 0.31773416,  1.8804253 ,  1.21070731],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 3.60199294, -1.29040027,  3.14699365],\n",
       "        [ 0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL.fit_[0]['beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "760ffca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "subsets =  list(combinations(list(range(10)), 3))\n",
    "asdf = []\n",
    "for subset in subsets:\n",
    "    asdf.append(GGL._L1penalty(np.linalg.inv(dg_M[:,:,subset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "55acf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "??GGL._L1penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3a58e5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmin_indices(lst):\n",
    "    min_value = min(lst)  # Find the minimum value in the list\n",
    "    return [i for i, x in enumerate(lst) if x == min_value]  # List indices where the element is the minimum value\n",
    "\n",
    "argmin_indices(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7f6a4269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 8)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "80b7c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "subsets =  list(combinations(list(range(10)), 3))\n",
    "asdf = []\n",
    "for subset in subsets:\n",
    "    asdf.append(GGL._L1penalty(dg_M[:,:,subset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8f546998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2634408767807169"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2dafe73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "subsets =  list(combinations(list(range(10)), 3))\n",
    "asdf = []\n",
    "for subset in subsets:\n",
    "    asdf.append(GGL._L1penalty(np.transpose(np.linalg.inv(dg_M[:,:,subset]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d3154996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.193900509597373"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0_npm = GGL.fit_[-1]['beta']\n",
    "coeffs[probe_init_low] = GGL.fit_[-1]['beta']\n",
    "combined_norms[probe_init_low] = np.sqrt((np.linalg.norm(coeffs[probe_init_low], axis = 0)**2).sum(axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72523e",
   "metadata": {},
   "source": [
    "You can express any $x_j = \\gamma^T x_{[d]}$ where $x_{[d]} \\in R^{d \\times d}$.\n",
    "\n",
    "Can we express certain $x_j$ more cheaply i.e. with $\\|\\gamma\\|_2 < 1$? No, not always (think about |<).\n",
    "\n",
    "But even in that case the ultimate selection is d-sparse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8570b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (montlake)",
   "language": "python",
   "name": "montlake_xxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
